{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"./images/btp-banner.gif\" alt=\"BTP A&C\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieval-Augmented Generation with SAP HANA Cloud Vector Engine\n",
    "\n",
    "In this demo, we will explore how to build **Knowledge Graphs (KGs)** from **tabular data**. Specifically, we will use tables **S013** and **LFA1** from the **SPURCHASE** schema to construct an **ontology** that captures the semantic relationships between these tables. The ontology will define key aspects such as table relationships, join conditions, and relevant columns. This semantic layer serves as the foundation for generating knowledge graphs, enabling more meaningful data integration and querying. Once the ontology is created, we will import it into **SAP HANA Cloud** to generate the Knowledge Graphs. The import can be done either via a local file or through supported cloud storage options. \n",
    "\n",
    "With the ontology and knowledge graph in place, we aim to **answer complex business or analytical questions** by leveraging the ontologies created and stored within the **SAP HANA Cloud database**. These ontologies provide a **semantic layer** that describes the structure, meaning, and relationships of the data in a machine-readable format, enabling more intelligent and context-aware data access. By using this semantic model, we can translate user queries or application logic into meaningful insights, even when the data resides in multiple interrelated tables. This approach enhances data **discovery, reasoning, and integration**, and supports more intuitive querying through graph-based models rather than complex joins or manual interpretation of raw table structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØLearning Objectives\n",
    "By the end of this demo, you will be able to:\n",
    "- Design and define ontologies using RDF/OWL standards to model business entities and their relationships.\n",
    "- Construct and serialize RDF graphs using Python libraries (e.g., rdflib) and export them in Turtle (.ttl) format.\n",
    "- Load and persist knowledge graphs into SAP HANA Cloud and validate their structure using SPARQL queries.\n",
    "- Perform semantic retrieval by writing SPARQL queries to extract relevant metadata based on user intent.\n",
    "- Map natural language questions to structured data by analyzing intent and aligning with metadata semantics.\n",
    "- Generate SQL queries dynamically using insights from the knowledge graph to answer complex business questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üö®Requirements\n",
    "\n",
    "Please review the following requirements before starting the demo: \n",
    "- Enable the additional feature **Triple Store** in your SAP HANA Cloud database \n",
    "- Deploy Large Language Models (LLMs): **anthropic--claude-3.5-sonnet** in SAP AI Launchpad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìùAbout the Data\n",
    "This Dataset is a **simulated version of vendor data** from **SAP S/4HANA**, created for demonstration purposes. It includes **vendor evaluation records (S013)** and **supplier master data (LFA1)**, which are essential for assessing vendor performance, identifying potential supply chain risks, and supporting data-driven procurement decisions.\n",
    "\n",
    "For the demo calculation of risk score on table S013, we used the following weighting key (sums to 1.00):\n",
    "|Criterion (column)\t|Description\t|Weight |\n",
    "|-------------------|---------------|-------|\n",
    "|**PWMT1**\t|Quantity-reliability points\t|0.15|\n",
    "|**PWTT1**\t|On-time-delivery points\t|0.15|\n",
    "|**PWEV1**\t|Compliance with shipping instructions\t|0.10|\n",
    "|**PWWE1**\t|Service-quality points\t|0.15|\n",
    "|**PWFR1**\t|Service timeliness\t|0.10|\n",
    "|**PWQA1**\t|Quality-audit points\t|0.10|\n",
    "|**LAVI1**\t|Shipping-notification performance\t|0.15|\n",
    "|**ALAV1**\t|Variance from shipping notification\t|0.10|\n",
    "|**Total**\t|‚Äî\t|1.00|\n",
    "\n",
    "- **VENDOR_SCORE** = Œ£( score √ó weight ) / 1.00  \n",
    "- **RISK_SCORE**   = 100 ‚Äì VENDOR_SCORE\n",
    "\n",
    "> **Important**: \n",
    "> - These weights are just an example. In a real SAP system they must mirror the weighting key defined in SPRO ‚ñ∏ Materials Management ‚ñ∏ Purchasing ‚ñ∏ Vendor Evaluation ‚ñ∏ Define Weighting Keys.\n",
    "> - If your system uses the standard key 000 (equal importance) you would set each weight to 0.125 (12.5 %) or simply average the eight criteria.\n",
    "> - Management dashboards often base the overall vendor rating on the ‚Äú‚Ä¶2‚Äù values to avoid wild month-to-month swings.\n",
    "> - If you load true historical S013 data (several periods) into HANA, switching the formula to the ‚Äú‚Ä¶2‚Äù fields will give exactly the same logic SAP uses in the standard report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Install Python Packages  \n",
    "Run the following package installations. **pip** is the package installer for Python. You can use pip to install packages from the Python Package Index and other indexes. \n",
    "\n",
    "‚ö†Ô∏è**Note:** Jupyter Notebook kernel restart required after package installation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rdflib --break-system-packages\n",
    "%pip install hdbcli --break-system-packages\n",
    "%pip install generative-ai-hub-sdk[all] --break-system-packages\n",
    "%pip install langchain-core --break-system-packages\n",
    "%pip install pandas --break-system-packages\n",
    "%pip install xml.etree --break-system-packages\n",
    "%pip install python-dotenv --break-system-packages\n",
    "# kernel restart required!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Initialize the RDF graph and add table metadata into the graph\n",
    "This code snippet demonstrates how to initialize an RDF graph using a Python library such as RDFLib, and how to construct the graph by extracting metadata from database tables, such as table names, column names, data types, and relationships (e.g., foreign key constraints). This metadata is then translated into RDF triples, capturing the structure and semantics of the underlying tabular data. The resulting RDF graph serves as the foundation for building ontologies or knowledge graphs, enabling semantic reasoning and enhanced data integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Literal, Namespace, RDF, RDFS, XSD\n",
    "\n",
    "# Define custom namespaces for our RDF graph\n",
    "ns = Namespace(\"http://supplychain_database.org/spurchase/\")  # Namespace for supply chain data\n",
    "db = Namespace(\"http://supplychain_database.org/database/\")  # Namespace for database schema\n",
    "\n",
    "# Create an empty RDF graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind namespace prefixes for cleaner serialization\n",
    "g.bind(\"spurchase\", ns)  # Associates \"spurchase\" prefix with our namespace\n",
    "g.bind(\"db\", db)       # Associates \"db\" prefix with database namespace\n",
    "\n",
    "# Define table resources\n",
    "s013 = ns.S013      # Resource for vendor evaluation table\n",
    "lfa1 = ns.LFA1  # Resource for vender metadata table\n",
    "\n",
    "# Add metadata for S013 table\n",
    "g.add((s013, RDF.type, db.Table))            # Set type as Table\n",
    "g.add((s013, RDFS.label, Literal(\"Vendor Risk Evaluations\")))  # Human-readable label\n",
    "g.add((s013, db.tableName, Literal(\"S013\"))) # Actual table name in database\n",
    "\n",
    "# Add metadata for LFA1 table\n",
    "g.add((lfa1, RDF.type, db.Table))            # Set type as Table\n",
    "g.add((lfa1, RDFS.label, Literal(\"Vendor Details\")))  # Human-readable label\n",
    "g.add((lfa1, db.tableName, Literal(\"LFA1\"))) # Actual table name in database\n",
    "\n",
    "print(\"Your knowledge graph is initialized!‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Define columns and metadata for Table S013\n",
    "The following code snippet constructs an RDF graph by extracting metadata from the database table S013, which is typically used for storing vendor evaluation data. The extracted metadata includes column names, column descriptions, applicable aggregation methods, and relationships to other tables, such as those defined by foreign key constraints. This metadata is then transformed into RDF triples, where each piece of information is represented as a semantic statement. By doing so, the graph captures not only the structure but also the contextual meaning of the data within S013, forming an essential component of a broader knowledge graph for enhanced data integration, reasoning, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns and metadata for S013 table\n",
    "s013_columns = {\n",
    "    # Client column metadata\n",
    "    ns.MANDT: {\n",
    "        \"label\": \"Client\",\n",
    "        \"isKey\": True,  # Mark as primary key\n",
    "        \"description\": \"Client identifier\"\n",
    "    },\n",
    "    # Vendor ID column metadata (foreign key)\n",
    "    ns.LIFNR: {\n",
    "        \"label\": \"Vendor ID\",\n",
    "        \"isKey\": True,  # Part of composite key\n",
    "        \"foreignKey\": ns.LIFNR,  # References LFA1.LIFNR\n",
    "        \"groupBy\": True,  # Can be used for grouping\n",
    "        \"description\": \"Foreign key to LFA1.LIFNR\"\n",
    "    },\n",
    "    # Material Number column metadata\n",
    "    ns.MATNR: {\n",
    "        \"label\": \"Material Number\",\n",
    "        \"isKey\": True,  # Part of composite key\n",
    "        \"groupBy\": True,  # Can be used for grouping\n",
    "        \"description\": \"Product identifier\"\n",
    "    },\n",
    "    # Month column metadata\n",
    "    ns.SPMON: {\n",
    "        \"label\": \"Month\",\n",
    "        \"groupBy\": True,  # Can be used for grouping\n",
    "        \"description\": \"Month to analyze (NVARCHAR format)\"\n",
    "    },\n",
    "    # Purchasing Organization column metadata\n",
    "    ns.EKORG: {\n",
    "        \"label\": \"Purchasing Organization\",\n",
    "        \"aggregation\": db.COUNT,  # Can be used with COUNT function\n",
    "        \"description\": \"Organization identifier\"\n",
    "    },\n",
    "    # Quantity Reliability column metadata\n",
    "    ns.PWMT1: {\n",
    "        \"label\": \"Quantity Reliability Score\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score (1-100) for quantity reliability\"\n",
    "    },\n",
    "    # On-time Delivery column metadata\n",
    "    ns.PWTT1: {\n",
    "        \"label\": \"On-time Delivery Score\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score (1-100) for on-time delivery performance\"\n",
    "    },\n",
    "    # Shipping Compliance column metadata\n",
    "    ns.PWEV1: {\n",
    "        \"label\": \"Shipping Compliance Score\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score (1-100) for compliance with shipping instructions\",\n",
    "    },\n",
    "    # Service Quality column metadata\n",
    "    ns.PWWE1: {\n",
    "        \"label\": \"Service Quality Score\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score (1-100) for service quality\",\n",
    "    },\n",
    "    # Service Timeliness column metadata\n",
    "    ns.PWFR1: {\n",
    "        \"label\": \"Service Timeliness Score\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score (1-100) for service timeliness\",\n",
    "    },\n",
    "    # Quality Audit column metadata\n",
    "    ns.PWQA1: {\n",
    "        \"label\": \"Quality Audit Score\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score (1-100) for quality audit\",\n",
    "    },\n",
    "    # Shipping Notification column metadata\n",
    "    ns.LAVI1: {\n",
    "        \"label\": \"Shipping Notification Score\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score (1-100) for shipping notification\",\n",
    "    },\n",
    "    # Variance from Shipping Notification column metadata\n",
    "    ns.ALAV1: {\n",
    "        \"label\": \"Variance from Shipping Notification Score\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score (1-100) for variance from shipping notification\",\n",
    "    },\n",
    "    # Vendor Reliability column metadata\n",
    "    ns.RELIA: {\n",
    "        \"label\": \"Vendor Reliability Score\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Vendor reliability score (1-100)\",\n",
    "    },\n",
    "    # Vendor Risk column metadata\n",
    "    ns.RISK1: {\n",
    "        \"label\": \"Vendor Risk Score\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Vendor risk score (1-100)\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add all S013 columns to the graph\n",
    "for col, meta in s013_columns.items():\n",
    "    # Basic column metadata\n",
    "    g.add((col, RDF.type, db.Column))  # Set type as Column\n",
    "    g.add((col, RDFS.label, Literal(meta[\"label\"])))  # Human-readable label\n",
    "    g.add((col, db.columnName, Literal(col.split(\"/\")[-1])))  # Extract column name from URI\n",
    "    g.add((col, db.description, Literal(meta[\"description\"])))  # Description\n",
    "\n",
    "    # Conditional metadata additions\n",
    "    if meta.get(\"isKey\"):\n",
    "        g.add((col, db.isPrimaryKey, Literal(True)))  # Mark as primary key\n",
    "    if meta.get(\"groupBy\"):\n",
    "        g.add((col, db.groupBy, Literal(True)))  # Mark as groupable\n",
    "    if meta.get(\"aggregation\"):\n",
    "        g.add((col, db.aggregationFunction, meta[\"aggregation\"]))  # Add aggregation function\n",
    "    # if meta.get(\"filter\"):\n",
    "    #    g.add((col, db.filterFunction, Literal(meta[\"filter\"])))  # Add filter function\n",
    "    if meta.get(\"foreignKey\"):\n",
    "        g.add((col, db.foreignKey, meta[\"foreignKey\"]))  # Add foreign key reference\n",
    "\n",
    "print(\"‚úÖTable S013 has been added to your knowledge graph.\")\n",
    "print(f\"üëçThere are {len(g)} triples in your knowledge graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Define columns and metadata for Table LFA1\n",
    "The following code snippet constructs an RDF graph by programmatically extracting metadata from the database table LFA1, which typically contains vendor master data. This metadata includes details such as column names, column descriptions, data types, potential aggregation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns and metadata for LFA1 table\n",
    "lfa1_columns = {\n",
    "    # Vendor ID column metadata (foreign key)\n",
    "    ns.LIFNR: {\n",
    "        \"label\": \"Vendor ID\",\n",
    "        \"isKey\": True,  # Primary key\n",
    "        \"description\": \"Primary key for vendor\"\n",
    "    },\n",
    "    # Vendor name column metadata\n",
    "    ns.NAME1: {\n",
    "        \"label\": \"Vendor Name\",\n",
    "        \"description\": \"Full name of vendor\"\n",
    "    },\n",
    "    # Country Key column metadata\n",
    "    ns.LAND1: {\n",
    "        \"label\": \"Country Key\",\n",
    "        \"description\": \"Country code of vendor (ISO 3166-1 alpha-2)\"\n",
    "    },\n",
    "    # City column metadata\n",
    "    ns.ORT01: {\n",
    "        \"label\": \"City\",\n",
    "        \"description\": \"City of vendor\"\n",
    "    },\n",
    "    # Address column metadata\n",
    "    ns.STRAS: {\n",
    "        \"label\": \"Vendor Address\",\n",
    "        \"description\": \"House number and street name of vendor\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add all LFA1 columns to the graph\n",
    "for col, meta in lfa1_columns.items():\n",
    "    # Basic column metadata\n",
    "    g.add((col, RDF.type, db.Column))  # Set type as Column\n",
    "    g.add((col, RDFS.label, Literal(meta[\"label\"])))  # Human-readable label\n",
    "    g.add((col, db.columnName, Literal(col.split(\"/\")[-1])))  # Extract column name from URI\n",
    "    g.add((col, db.description, Literal(meta[\"description\"])))  # Description\n",
    "    # Conditional metadata additions\n",
    "    if meta.get(\"isKey\"):\n",
    "        g.add((col, db.isPrimaryKey, Literal(True)))  # Mark as primary key\n",
    "\n",
    "print(\"‚úÖTable LFA1 has been added to your knowledge graph.\")\n",
    "print(f\"üëçThere are {len(g)} triples in your knowledge graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Define relationships between tables\n",
    "The following code snippet constructs an RDF graph by explicitly defining the semantic relationships between the two database tables, S013 and LFA1. It establishes how these tables are linked‚Äîtypically through foreign keys or logical associations‚Äîand represents these connections using RDF triples. Each triple encodes a subject-predicate-object relationship, allowing the graph to capture the meaning and structure of the data. This forms the basis for integrating relational data into a knowledge graph, enabling advanced querying, reasoning, and data interoperability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relationships between tables\n",
    "g.add((s013, db.relatedTo, lfa1))  # General relationship between tables\n",
    "\n",
    "# Explicit foreign key relationship\n",
    "g.add((ns.LIFNR, db.foreignKey, ns.LIFNR))  # S013.LIFNR ‚Üí LFA1.LIFNR\n",
    "\n",
    "# Join condition for the relationship\n",
    "g.add((ns.LIFNR, db.joinCondition, Literal(\"S013.MANDT = LFA1.MANDT AND S013.LIFNR = LFA1.LIFNR\")))\n",
    "\n",
    "print(\"‚úÖTable relations have been added to your knowledge graph.\")\n",
    "print(f\"üëçThere are {len(g)} triples in your knowledge graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Serialize the graph in Turtle (Terse RDF Triple Language) format\n",
    "\n",
    "The Turtle data format is a way of representing data using the RDF. It's a form of serializing RDF data in a human-readable and easy-to-write format. Turtle is defined by the W3C and uses a syntax like the N-Triples format, with added support for prefixes and shorthand notations to make it more concise and readable. Turtle is commonly used for writing and sharing RDF data on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the graph to Turtle format\n",
    "graph_string = g.serialize(format=\"turtle\")\n",
    "\n",
    "# Write the Turtle string to a file\n",
    "with open('./spurchase_tabular.ttl', 'w') as file:\n",
    "    file.write(graph_string)\n",
    "\n",
    "print(\"‚úÖTurtle file has been generated locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 7: Connect to SAP HANA Cloud database\n",
    "\n",
    "The provided Python script imports database connection modules and initiates a connection to a SAP HANA Cloud instance using the `dbapi` module. The user is prompted to enter their username and password, which are then used to establish a secure connection to the SAP HANA Cloud database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up HANA Cloud Connection to import the ttl file\n",
    "from hdbcli import dbapi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "# Get the HANA Cloud username from environment variables\n",
    "HANA_USER = os.getenv('HANA_VECTOR_USER')\n",
    "# Get the HANA Cloud password from environment variables\n",
    "HANA_PASS = os.getenv('HANA_VECTOR_PASS')\n",
    "# Get the HANA Cloud host from environment variables\n",
    "HANA_HOST = os.getenv('HANA_VECTOR_HOST')\n",
    "\n",
    "# Establish connection to SAP HANA Cloud database\n",
    "conn = dbapi.connect(\n",
    "    user = HANA_USER,\n",
    "    password = HANA_PASS,\n",
    "    address = HANA_HOST,\n",
    "    port = 443,\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"‚úÖHANA Cloud connection is established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 8: Import the Turtle file into SAP HANA Cloud\n",
    "Finally, the Turtle file‚Äîcontaining structured RDF triples that define the semantic relationships between entities‚Äîwill be imported into the SAP HANA Cloud database, where it will be stored and represented as a knowledge graph. This process enables the underlying data to be queried and analyzed in a graph-based format, allowing for advanced semantic reasoning, relationship exploration, and integration with SAP HANA‚Äôs native graph processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the ttl file into SAP HANA Cloud\n",
    "ttl_filename = \"./spurchase_tabular.ttl\"\n",
    "graphname = \"spurchase_graph_\" + HANA_USER\n",
    "try:\n",
    "    with open(ttl_filename, 'r') as ttlfp:\n",
    "        request_hdrs = ''\n",
    "        request_hdrs += 'rqx-load-protocol: true' + '\\r\\n'            # required header for upload protocol\n",
    "        request_hdrs += 'rqx-load-filename: ' + ttl_filename + '\\r\\n' # optional header\n",
    "        request_hdrs += 'rqx-load-graphname: ' + graphname + '\\r\\n'   # optional header to specify name of the graph, if not provided RDF data will be loaded to internal-default-graph\n",
    "        conn.cursor().callproc('SPARQL_EXECUTE', (ttlfp.read(), request_hdrs, '', None))\n",
    "\n",
    "    print(\"‚úÖSuccess! The RDF graph has been successfully ingested into SAP HANA Cloud as graph:\", graphname)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùåError occurred while ingesting the graph:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 9: Validate the Knowledge Graph in SAP HANA Cloud\n",
    "We will perform queries on the ontologies stored in the SAP HANA Cloud database to explore and validate the semantic structures that have been generated from the underlying tabular data. These queries will help verify that the RDF triples or graph representations correctly model the relationships, hierarchies, and attributes as defined in the original database schema. By doing so, we can ensure that the **graph structure** not only faithfully mirrors the actual tabular data‚Äîsuch as tables, columns, and foreign key constraints‚Äîbut also captures the **intended semantic relationships** between different entities. This validation step is crucial to confirm the **consistency, completeness, and integrity** of the knowledge graph before it is used for advanced semantic analysis, data integration, or reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all imported ontologies\n",
    "validation_query = \"\"\"\n",
    "SELECT ?graph (COUNT(*) as ?num_triples)\n",
    "WHERE {\n",
    "    GRAPH ?graph { [?p []] }\n",
    "}\n",
    "GROUP BY ?graph\n",
    "ORDER BY ?graph\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SPARQL query\n",
    "try:\n",
    "    resp = cursor.callproc('SPARQL_EXECUTE', (\n",
    "        validation_query,\n",
    "        'Accept: application/sparql-results+csv',\n",
    "        '?',\n",
    "        None\n",
    "    ))\n",
    "    # metadata = resp[3]\n",
    "    # results = resp[2]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Validation Query Results:\")\n",
    "    print(resp[2])\n",
    "    # print(\"Response Metadata:\", metadata)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"SPARQL_EXECUTE failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following code to see all the triples from the imported knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the graph we imported\n",
    "\n",
    "validation_query = \"\"\"\n",
    "SELECT * WHERE {\n",
    "  GRAPH <\"\"\" + graphname + \"\"\"> {\n",
    "    ?s ?p ?o\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SPARQL query\n",
    "try:\n",
    "    resp = cursor.callproc('SPARQL_EXECUTE', (\n",
    "        validation_query,\n",
    "        'Accept: application/sparql-results+csv',\n",
    "        '?',\n",
    "        None\n",
    "    ))\n",
    "    # metadata = resp[3]\n",
    "    # results = resp[2]\n",
    "\n",
    "    # Print results\n",
    "    print(\"Validation Query Results:\")\n",
    "    print(resp[2])\n",
    "    # print(\"Response Metadata:\", metadata)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"SPARQL_EXECUTE failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 10: Configure AI Core Client\n",
    "Execute the configuration module below to enable access to SAP‚Äôs Generative AI foundation models. Running this code block will automatically handle the necessary setup, including authentication and environment configuration, to ensure seamless connectivity to the Generative AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "from gen_ai_hub.proxy.gen_ai_hub_proxy import GenAIHubProxyClient\n",
    "\n",
    "# Get the AI Core URL from environment variables\n",
    "URL = os.getenv('AICORE_AUTH_URL')\n",
    "# Get the AI Core client ID from environment variables\n",
    "CLIENT_ID = os.getenv('AICORE_CLIENT_ID')\n",
    "# Get the AI Core client secret from environment variables\n",
    "CLIENT_SECRET = os.getenv('AICORE_CLIENT_SECRET')\n",
    "# Get the AI Core client ID from environment variables\n",
    "RESOURCE_GROUP = os.getenv('AICORE_RESOURCE_GROUP')\n",
    "# Get the AI Core client secret from environment variables\n",
    "API_URL = os.getenv('AICORE_BASE_URL')\n",
    "\n",
    "# Set up the AICoreV2Client\n",
    "ai_core_client = AICoreV2Client(base_url=API_URL,\n",
    "                            auth_url=URL,\n",
    "                            client_id=CLIENT_ID,\n",
    "                            client_secret=CLIENT_SECRET,\n",
    "                            resource_group=RESOURCE_GROUP)\n",
    "\n",
    "# Initialize GenAIHub proxy client\n",
    "proxy_client = GenAIHubProxyClient(ai_core_client=ai_core_client)\n",
    "print(\"‚úÖAI Core Client connection is established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The large language model (LLM) is initialized as an instance of ChatBedrock using the **anthropic--claude-3.5-sonnet** model. This instance serves as the conversational interface, enabling the generation of context-aware responses and facilitating interactions in a chat-like environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.amazon import ChatBedrock\n",
    "# Initialize the ChatBedrock client with the proxy client\n",
    "anthropic = ChatBedrock(\n",
    "    model_name=\"anthropic--claude-3.7-sonnet\",\n",
    "    proxy_client=proxy_client # Pass the proxy client to ChatBedrock\n",
    ")\n",
    "print(\"‚úÖLLM model connection is established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 11: Extract Relevant Metadata using SPARQL\n",
    "This script executes a SPARQL query against a knowledge graph stored in SAP HANA Cloud, parses the XML response, and extracts the resulting RDF triples (subject, predicate, object) into a Python list called metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# Execute SPARQL query to get all relevant triples\n",
    "sparql_query = \"\"\"\n",
    "SELECT * WHERE {\n",
    "  GRAPH <\"\"\" + graphname + \"\"\"> {\n",
    "    ?s ?p ?o\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    resp = cursor.callproc('SPARQL_EXECUTE', (sparql_query, 'Metadata headers describing Input and/or Output', '?', None))\n",
    "    \n",
    "    if resp and len(resp) >= 3 and resp[2]:\n",
    "        # Parse the XML response\n",
    "        xml_response = resp[2]\n",
    "        try:\n",
    "            root = ET.fromstring(xml_response)\n",
    "            results = []\n",
    "            \n",
    "            for result in root.findall('.//{http://www.w3.org/2005/sparql-results#}result'):\n",
    "                row = {}\n",
    "                for binding in result:\n",
    "                    var_name = binding.attrib['name']\n",
    "                    value = binding[0]  # uri or literal\n",
    "                    if value.tag.endswith('uri'):\n",
    "                        row[var_name] = value.text\n",
    "                    elif value.tag.endswith('literal'):\n",
    "                        row[var_name] = value.text\n",
    "                results.append(row)\n",
    "        except ET.ParseError as e:\n",
    "            print(f\"Error parsing XML: {e}\")\n",
    "        # results = parse_sparql_results(xml_response)\n",
    "        \n",
    "        # Convert to our standard format\n",
    "        metadata = []\n",
    "        for row in results:\n",
    "            metadata.append({\n",
    "                's': row.get('s', ''),\n",
    "                'p': row.get('p', ''),\n",
    "                'o': row.get('o', '')\n",
    "            })\n",
    "\n",
    "    print(f\"‚úÖSPARQL query executed successfully! Retrieved {len(metadata)} triples.\")\n",
    "\n",
    "    # Print the metadata\n",
    "    for item in metadata:\n",
    "        print(item)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing SPARQL query: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 12: Analyze the Metadata and Natural Language Question\n",
    "This code snippet is designed to analyze a natural language question by leveraging metadata from a knowledge graph and using a large language model (LLM) (via LangChain and Anthropic) to extract relevant SQL components (tables, columns, filters, joins, etc.).\n",
    "\n",
    "Some other questions you can ask: \n",
    "1. What is the risk score for purchasing material 'MAT0151' from vendor '1011'?\n",
    "2. Find the vendors' name whose risk score is higher than 34 for the material MAT0151."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# Analyze the metadata to identify tables, columns, and relationships\n",
    "# Convert metadata to a format the LLM can understand\n",
    "metadata_str = \"\\n\".join([f\"{item['s']} {item['p']} {item['o']}\" for item in metadata])\n",
    "question = \"Find the vendors's name whose risk score is higher than 34 for the material MAT0151.\"\n",
    "\n",
    "prompt_template = \"\"\"Given the following RDF metadata about database tables and columns, analyze the user's question and identify:\n",
    "1. The main table(s) involved with their schema (SPURCHASE)\n",
    "2. The columns needed (including any aggregation functions)\n",
    "3. Any filters or conditions\n",
    "4. Any joins required\n",
    "\n",
    "Important Rules:\n",
    "- Always include the schema name (SPURCHASE) before table names\n",
    "- When using GROUP BY, include the grouping columns in SELECT\n",
    "- Never include any explanatory text in the SQL output\n",
    "- For country codes like Germany, use 'DE' in filters\n",
    "\n",
    "For each column, include:\n",
    "- The column name (prefix with table alias if needed)\n",
    "- Any aggregation function (AVG, COUNT, etc.)\n",
    "- Any filter conditions\n",
    "- Whether it's a grouping column\n",
    "\n",
    "For tables, include:\n",
    "- The full table name with schema (e.g., SPURCHASE.S013)\n",
    "- Any relationships to other tables\n",
    "\n",
    "\n",
    "Metadata:\n",
    "{metadata}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Return your analysis in this exact format (without any additional explanations):\n",
    "Tables: [schema1.table1, schema2.table2]\n",
    "Columns: [column1, column2, column3, column with aggregations like AVG(RISK1)]\n",
    "Filters: [filter condition1,filter condition2]\n",
    "Joins: [join condition1, join condition2]\n",
    "GroupBy: [columns1, columns2]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template).invoke({\n",
    "    \"metadata\": metadata_str,\n",
    "    \"question\": question\n",
    "})\n",
    "\n",
    "# We'll use the LLM to extract the key components\n",
    "analysis = anthropic.invoke(prompt)\n",
    "print(analysis.content)\n",
    "# return parse_analysis(analysis.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 13: Parse the LLM Response\n",
    "This code parses and structures the response from a Large Language Model (LLM), which analyzed a natural language question and metadata to return components for an SQL query (e.g., tables, columns, filters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a library to parse the analysis content\n",
    "components = {\n",
    "    \"tables\": [],\n",
    "    \"columns\": [],\n",
    "    \"filters\": [],\n",
    "    \"joins\": [],\n",
    "    \"group_by\": []\n",
    "}\n",
    "\n",
    "# Remove any \"Explanation:\" text\n",
    "analysis.content = analysis.content.split(\"Explanation:\")[0].strip()\n",
    "\n",
    "# Parse each section\n",
    "current_section = None\n",
    "for line in analysis.content.split('\\n'):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "        \n",
    "    if line.startswith('Tables:'):\n",
    "        current_section = 'tables'\n",
    "        tables = line.split(':')[1].strip()\n",
    "        components['tables'] = [t.strip() for t in tables.split(',') if t.strip()]\n",
    "    elif line.startswith('Columns:'):\n",
    "        current_section = 'columns'\n",
    "        cols = line.split(':')[1].strip()\n",
    "        for col_part in cols.split(','):\n",
    "            col_part = col_part.strip()\n",
    "            if col_part:\n",
    "                if '(' in col_part and ')' in col_part:\n",
    "                    agg = col_part.split('(')[0].strip().upper()\n",
    "                    col = col_part.split('(')[1].split(')')[0].strip()\n",
    "                    components['columns'].append((agg, col))\n",
    "                else:\n",
    "                    components['columns'].append((None, col_part))\n",
    "    elif line.startswith('Filters:'):\n",
    "        current_section = 'filters'\n",
    "        filters = line.split(':')[1].strip()\n",
    "        components['filters'] = [f.strip() for f in filters.split(',') if f.strip()]\n",
    "    elif line.startswith('Joins:'):\n",
    "        current_section = 'joins'\n",
    "        joins = line.split(':')[1].strip()\n",
    "        components['joins'] = [j.strip() for j in joins.split(',') if j.strip()]\n",
    "    elif line.startswith('GroupBy:'):\n",
    "        current_section = 'group_by'\n",
    "        group_bys = line.split(':')[1].strip()\n",
    "        components['group_by'] = [g.strip() for g in group_bys.split(',') if g.strip()]\n",
    "    elif current_section:\n",
    "        # Handle multi-line sections\n",
    "        if current_section == 'tables':\n",
    "            components['tables'].extend([t.strip() for t in line.split(',') if t.strip()])\n",
    "        elif current_section == 'columns':\n",
    "            for col_part in line.split(','):\n",
    "                col_part = col_part.strip()\n",
    "                if col_part:\n",
    "                    if '(' in col_part and ')' in col_part:\n",
    "                        agg = col_part.split('(')[0].strip().upper()\n",
    "                        col = col_part.split('(')[1].split(')')[0].strip()\n",
    "                        components['columns'].append((agg, col))\n",
    "                    else:\n",
    "                        components['columns'].append((None, col_part))\n",
    "        elif current_section == 'filters':\n",
    "            components['filters'].extend([f.strip() for f in line.split(',') if f.strip()])\n",
    "        elif current_section == 'joins':\n",
    "            components['joins'].extend([j.strip() for j in line.split(',') if j.strip()])\n",
    "        elif current_section == 'group_by':\n",
    "            components['group_by'].extend([g.strip() for g in line.split(',') if g.strip()])\n",
    "\n",
    "# Ensure schema is included in table names\n",
    "components['tables'] = [f\"SPURCHASE.{t.split('.')[-1]}\" if '.' not in t else t for t in components['tables']]\n",
    "\n",
    "# Ensure grouping columns are included in SELECT - CORRECTED VERSION\n",
    "for group_col in components['group_by']:\n",
    "    # Check if this exact (None, group_col) pair exists\n",
    "    col_exists = any(col == (None, group_col) for col in components['columns'])\n",
    "    # Check if group_col appears in any non-aggregated column reference\n",
    "    col_part_of_ref = any(group_col in col[1] for col in components['columns'] if col[0] is None)\n",
    "    \n",
    "    if not col_exists and not col_part_of_ref:\n",
    "        components['columns'].append((None, group_col))\n",
    "print(components)\n",
    "# return components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14: Generate SQL Query\n",
    "This code snipet takes the structured components extracted from a Large Language Model (LLM) and generates a clean, valid SQL query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clean SQL query from the analyzed components\"\"\"\n",
    "# Validate components\n",
    "if not components[\"tables\"]:\n",
    "    raise ValueError(\"No tables identified for SQL generation\")\n",
    "\n",
    "# Clean all components first\n",
    "def clean_component(component):\n",
    "    return component.replace('[', '').replace(']', '').strip()\n",
    "\n",
    "# Build SELECT clause - ensure GROUP BY columns are included\n",
    "select_parts = []\n",
    "\n",
    "# First add all GROUP BY columns to SELECT if they're not already there\n",
    "for group_col in components.get(\"group_by\", []):\n",
    "    group_col = clean_component(group_col)\n",
    "    if not any(col[1] == group_col for col in components[\"columns\"] if col[0] is None):\n",
    "        select_parts.append(group_col)\n",
    "\n",
    "# Then add the requested columns\n",
    "for agg, col in components[\"columns\"]:\n",
    "    col = clean_component(col)\n",
    "    if not col:\n",
    "        continue\n",
    "    if agg:\n",
    "        select_parts.append(f\"{agg}({col}) AS {agg}_{col}\")\n",
    "    else:\n",
    "        if col not in select_parts:  # Don't add duplicates\n",
    "            select_parts.append(col)\n",
    "\n",
    "if not select_parts:  # Default to all columns if none specified\n",
    "    select_parts.append(\"*\")\n",
    "\n",
    "select_clause = \", \".join(select_parts[1:])\n",
    "# print(\"SELECT BEFORE \"+select_clause)\n",
    "# print(select_parts)\n",
    "\n",
    "# Build FROM clause\n",
    "from_table = clean_component(components[\"tables\"][0])\n",
    "from_clause = from_table\n",
    "\n",
    "# Add joins only if they exist and are not empty\n",
    "join_clauses = []\n",
    "for join in components.get(\"joins\", []):\n",
    "    clean_join = clean_component(join)\n",
    "    if clean_join and clean_join != 'INNER JOIN':\n",
    "        join_clauses.append(f\"INNER JOIN SPURCHASE.LFA1 ON {clean_join}\")\n",
    "# print(\"INNER JOIN \"+clean_join)\n",
    "\n",
    "# Build WHERE clause\n",
    "where_clauses = []\n",
    "for filter_cond in components.get(\"filters\", []):\n",
    "    clean_filter = clean_component(filter_cond)\n",
    "    if clean_filter:\n",
    "        where_clauses.append(clean_filter)\n",
    "\n",
    "where_clause = \" AND \".join(where_clauses) if where_clauses else \"\"\n",
    "where_clause = where_clause.replace(\",\", \" AND\")\n",
    "# print(\"WHERE CLAUSE \"+where_clause)\n",
    "\n",
    "# Build GROUP BY clause\n",
    "group_by_columns = [clean_component(g) for g in components.get(\"group_by\", []) if clean_component(g)]\n",
    "group_by_clause = \", \".join(group_by_columns) if group_by_columns else \"\"\n",
    "\n",
    "# Construct the SQL\n",
    "sql = f\"SELECT {select_clause} FROM {from_clause}\"\n",
    "\n",
    "if join_clauses:\n",
    "    sql += \" \" + \" \".join(join_clauses)\n",
    "\n",
    "if where_clause:\n",
    "    sql += f\" WHERE {where_clause}\"\n",
    "\n",
    "if group_by_clause:\n",
    "    sql += f\" GROUP BY {group_by_clause}\"\n",
    "\n",
    "# Final formatting\n",
    "sql = sql.strip()\n",
    "if not sql.endswith(';'):\n",
    "    sql += ';'\n",
    "\n",
    "print(sql)\n",
    "#return sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 15: Execute the SQL Query\n",
    "This code executes a SQL query using a database connection (conn) and processes the result into a clean, tabular format using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Execute the generated SQL query and return results\n",
    "try:\n",
    "    cursor.execute(sql)\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "    results = pd.DataFrame(rows, columns=columns)\n",
    "    print(results)\n",
    "except Exception as e:\n",
    "    print(f\"Error executing SQL query: {e}\")\n",
    "finally:\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 16: Generate and Return the Final Response\n",
    "This code generates a natural language explanation of the SQL query results using a large language model (LLM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a natural language response from the query results\n",
    "if results.empty:\n",
    "    print(\"‚ùåNo results found for your query!\")\n",
    "\n",
    "prompt_template = \"\"\"Convert the following query results into a natural language response to the user's question. \n",
    "Keep the response concise but informative. Include relevant numbers and comparisons where appropriate.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Results:\n",
    "{results}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template).invoke({\n",
    "    \"question\": question,\n",
    "    \"results\": results.to_string()\n",
    "})\n",
    "\n",
    "response = anthropic.invoke(prompt)\n",
    "print(response.content)\n",
    "print(\"üíØThis is the end of the demo. Thank you for your attention!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
