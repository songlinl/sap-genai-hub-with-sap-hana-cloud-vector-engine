{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26d0d51",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"./images/btp-banner.gif\" alt=\"BTP A&C\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa8e5be",
   "metadata": {},
   "source": [
    "## Hybrid RAG with SAP HANA Cloud Vector Engine and Knowledge Graph Engine\n",
    "In this demo, we will explore how to build hybrid RAG (Retrieval-Augmented Generation) using SAP HANA Cloud Vector Engine and Knowledge Graph Engine. Hybrid RAG is a modern and powerful approach to improving the quality of answers generated by large language models (LLMs). This hybrid approach enables the LLM to generate more accurate, comprehensive, and trustworthy responses, making it especially valuable in enterprise applications, customer support, healthcare, legal, and scientific domains where both nuance and factual accuracy matter. You will learn how to use Vector similarity search and SPARQL queries to retrieve and analyze supplier information from both unstructured and structured data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7cd454",
   "metadata": {},
   "source": [
    "## üéØLearning Objectives\n",
    "By the end of this demo, you will be able to:\n",
    "- Design and implement a dual-retrieval strategy where both vector similarity search and SPARQL queries extract relevant context for a user query.\n",
    "- Integrate and orchestrate responses by combining vector-retrieved content and knowledge graph-derived metadata to formulate precise and context-aware LLM prompts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0e2f21",
   "metadata": {},
   "source": [
    "## üö®Requirements\n",
    "\n",
    "Please review the following requirements before starting the demo: \n",
    "- Complete the notebook **Retrieval-Augmented Generation with SAP HANA Cloud Vector Engine**\n",
    "- Complete the notebook **Retrieval-Augmented Generation with SAP HANA Cloud Knowledge Graph**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d94255",
   "metadata": {},
   "source": [
    "### Step 1: Install Python packages\n",
    "\n",
    "Run the following package installations. **pip** is the package installer for Python. You can use pip to install packages from the Python Package Index and other indexes.\n",
    "\n",
    "‚ö†Ô∏è**Note:** Jupyter Notebook kernel restart required after package installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee135f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install generative-ai-hub-sdk[all] --break-system-packages\n",
    "%pip install hdbcli --break-system-packages\n",
    "%pip install langchain-core --break-system-packages\n",
    "%pip install langchain-hana --break-system-packages\n",
    "%pip install pandas --break-system-packages\n",
    "%pip install xml.etree --break-system-packages\n",
    "%pip install python-dotenv --break-system-packages\n",
    "\n",
    "# kernel restart required!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bc2f86",
   "metadata": {},
   "source": [
    "### Step 2: Configure AI Core Client\n",
    "Execute the configuration module below to enable access to SAP‚Äôs Generative AI foundation models. Running this code block will automatically handle the necessary setup, including authentication and environment configuration, to ensure seamless connectivity to the Generative AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c553651",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "from gen_ai_hub.proxy.gen_ai_hub_proxy import GenAIHubProxyClient\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Get the AI Core URL from environment variables\n",
    "URL = os.getenv('AICORE_AUTH_URL')\n",
    "# Get the AI Core client ID from environment variables\n",
    "CLIENT_ID = os.getenv('AICORE_CLIENT_ID')\n",
    "# Get the AI Core client secret from environment variables\n",
    "CLIENT_SECRET = os.getenv('AICORE_CLIENT_SECRET')\n",
    "# Get the AI Core client ID from environment variables\n",
    "RESOURCE_GROUP = os.getenv('AICORE_RESOURCE_GROUP')\n",
    "# Get the AI Core client secret from environment variables\n",
    "API_URL = os.getenv('AICORE_BASE_URL')\n",
    "\n",
    "# Set up the AICoreV2Client\n",
    "ai_core_client = AICoreV2Client(base_url=API_URL,\n",
    "                            auth_url=URL,\n",
    "                            client_id=CLIENT_ID,\n",
    "                            client_secret=CLIENT_SECRET,\n",
    "                            resource_group=RESOURCE_GROUP)\n",
    "\n",
    "# Initialize GenAIHub proxy client\n",
    "proxy_client = GenAIHubProxyClient(ai_core_client=ai_core_client)\n",
    "print(\"‚úÖAI Core Client connection is established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767faa0",
   "metadata": {},
   "source": [
    "Initialize the embedding and LLM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59414dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize embedding and LLM models\n",
    "from gen_ai_hub.proxy.langchain import OpenAIEmbeddings, ChatOpenAI, ChatBedrock\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(proxy_model_name='text-embedding-ada-002', proxy_client=proxy_client)\n",
    "gpt_model = ChatOpenAI(proxy_model_name='gpt-4o', proxy_client=proxy_client)\n",
    "anthropic_model = ChatBedrock(model_name=\"anthropic--claude-3.7-sonnet\", proxy_client=proxy_client)\n",
    "print(\"‚úÖAI models are initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c98720",
   "metadata": {},
   "source": [
    "### Step 3: Connect to SAP HANA Cloud database\n",
    "\n",
    "The provided Python script imports database connection modules and initiates a connection to a SAP HANA Cloud instance using the **dbapi** module. The user is prompted to enter their username and password, which are then used to establish a secure connection to the SAP HANA Cloud database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf8533",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up HANA Cloud Connection\n",
    "from hdbcli import dbapi\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "# Get the HANA Cloud username from environment variables\n",
    "HANA_USER = os.getenv('HANA_VECTOR_USER')\n",
    "# Get the HANA Cloud password from environment variables\n",
    "HANA_PASS = os.getenv('HANA_VECTOR_PASS')\n",
    "# Get the HANA Cloud host from environment variables\n",
    "HANA_HOST = os.getenv('HANA_VECTOR_HOST')\n",
    "\n",
    "# Establish connection to SAP HANA Cloud database\n",
    "conn = dbapi.connect(\n",
    "    user = HANA_USER,\n",
    "    password = HANA_PASS,\n",
    "    address = HANA_HOST,\n",
    "    port = 443,\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"‚úÖHANA Cloud connection is established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6e9acf",
   "metadata": {},
   "source": [
    "Create a LangChain VectorStore interface for the HANA database and specify the table (collection) to use for accessing the vector embeddings. Embeddings are vector representations of text data that incorporate the semantic meaning of the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a999645",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_hana import HanaDB\n",
    "\n",
    "#Create a LangChain VectorStore interface for the HANA database and specify the table (collection) to use for accessing the vector embeddings\n",
    "db_vec_table = HanaDB(\n",
    "    embedding=embedding_model, \n",
    "    connection=conn, \n",
    "    table_name=\"PRODUCTS_IT_ACCESSORY_ADA_\"+ HANA_USER,\n",
    "    content_column=\"VEC_TEXT\", # the original text description of the product details\n",
    "    metadata_column=\"VEC_META\", # metadata associated with the product details\n",
    "    vector_column=\"VEC_VECTOR\" # the vector representation of each product \n",
    ")\n",
    "print(\"‚úÖVector Database connection is established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1852b1d4",
   "metadata": {},
   "source": [
    "### Step 4: Complex Queries\n",
    "A sample question that queries both vector database and tabular data from a Knowledge Graph.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e286f3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"I want to see all suppliers that sell Logitech keyboard with product rating higher than 4-star, and have a risk score lower than 20.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4a81f",
   "metadata": {},
   "source": [
    "### Step 5: Retrieve context from SAP HANA Cloud Vector Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7b5c81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve vector context\n",
    "def retrieve_vector_context(question, top_k=25):\n",
    "    retriever = db_vec_table.as_retriever(search_kwargs={'k': top_k})\n",
    "    return retriever.invoke(question)\n",
    "\n",
    "vector_context = retrieve_vector_context(question)\n",
    "print(\"Vector Context Retrieved:\")\n",
    "print(vector_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12110e9",
   "metadata": {},
   "source": [
    "### Step 6: Retrieve tabular data from SAP HANA Cloud Knowledge Graph Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05544d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from typing import Dict, List\n",
    "from xml.etree import ElementTree as ET\n",
    "import pandas as pd\n",
    "\n",
    "def extract_metadata(conn) -> List[Dict]:\n",
    "    \"\"\"Extract relevant metadata from RDF triples using SPARQL\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    \n",
    "    try:\n",
    "        # Execute SPARQL query to get all relevant triples\n",
    "        graph_uri = \"spurchase_graph_\" + HANA_USER\n",
    "        sparql_query = \"\"\"\n",
    "        SELECT * WHERE {\n",
    "        GRAPH <\"\"\" + graph_uri + \"\"\"> {\n",
    "            ?s ?p ?o\n",
    "        }\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        resp = cursor.callproc('SPARQL_EXECUTE', (sparql_query, 'Metadata headers describing Input and/or Output', '?', None))\n",
    "        \n",
    "        if resp and len(resp) >= 3 and resp[2]:\n",
    "            # Parse the XML response\n",
    "            xml_response = resp[2]\n",
    "            results = parse_sparql_results(xml_response)\n",
    "            \n",
    "            # Convert to our standard format\n",
    "            metadata = []\n",
    "            for row in results:\n",
    "                metadata.append({\n",
    "                    's': row.get('s', ''),\n",
    "                    'p': row.get('p', ''),\n",
    "                    'o': row.get('o', '')\n",
    "                })\n",
    "            return metadata\n",
    "        return []\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SPARQL query: {e}\")\n",
    "        return []\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "def analyze_metadata(metadata: List[Dict], question: str, anthropic) -> Dict:\n",
    "    \"\"\"Analyze the metadata to identify tables, columns, and relationships\"\"\"\n",
    "    # Convert metadata to a format the LLM can understand\n",
    "    metadata_str = \"\\n\".join([f\"{item['s']} {item['p']} {item['o']}\" for item in metadata])\n",
    "    \n",
    "    prompt_template = \"\"\"Given the following RDF metadata about database tables and columns, analyze the user's question and identify:\n",
    "    1. The main table(s) involved with their schema (SPURCHASE)\n",
    "    2. The columns needed (including any aggregation functions)\n",
    "    3. Any filters or conditions\n",
    "    4. Any joins required\n",
    "\n",
    "    Important Rules:\n",
    "    - Always include the schema name (SPURCHASE) before table names\n",
    "    - When using GROUP BY, include the grouping columns in SELECT\n",
    "    - Never include any explanatory text in the SQL output\n",
    "    - For country codes like Germany, use 'DE' in filters\n",
    "    - Ignore any product names or descriptions\n",
    "    - Ignore any product ratings or reviews\n",
    "\n",
    "    For each column, include:\n",
    "    - The column name (prefix with table alias if needed)\n",
    "    - Any aggregation function (AVG, COUNT, etc.)\n",
    "    - Any filter conditions\n",
    "    - Whether it's a grouping column\n",
    "\n",
    "    For tables, include:\n",
    "    - The full table name with schema (e.g., SPURCHASE.S013)\n",
    "    - Any relationships to other tables\n",
    "\n",
    "\n",
    "    Metadata:\n",
    "    {metadata}\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Return your analysis in this exact format (without any additional explanations):\n",
    "    Tables: [schema1.table1, schema2.table2]\n",
    "    Columns: [column1, column2, column3, column with aggregations like AVG(RISK1)]\n",
    "    Filters: [filter condition1,filter condition2]\n",
    "    Joins: [join condition1, join condition2]\n",
    "    GroupBy: [columns1Ôºå columns2]\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(prompt_template).invoke({\n",
    "        \"metadata\": metadata_str,\n",
    "        \"question\": question\n",
    "    })\n",
    "    \n",
    "    # We'll use the LLM to extract the key components\n",
    "    analysis = anthropic_model.invoke(prompt)\n",
    "    return parse_analysis(analysis.content)\n",
    "\n",
    "def parse_analysis(analysis_text: str) -> Dict:\n",
    "    \"\"\"Parse the LLM's analysis into a structured format\"\"\"\n",
    "    components = {\n",
    "        \"tables\": [],\n",
    "        \"columns\": [],\n",
    "        \"filters\": [],\n",
    "        \"joins\": [],\n",
    "        \"group_by\": []\n",
    "    }\n",
    "    \n",
    "    # Remove any \"Explanation:\" text\n",
    "    analysis_text = analysis_text.split(\"Explanation:\")[0].strip()\n",
    "    \n",
    "    # Parse each section\n",
    "    current_section = None\n",
    "    for line in analysis_text.split('\\n'):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "            \n",
    "        if line.startswith('Tables:'):\n",
    "            current_section = 'tables'\n",
    "            tables = line.split(':')[1].strip()\n",
    "            components['tables'] = [t.strip() for t in tables.split(',') if t.strip()]\n",
    "        elif line.startswith('Columns:'):\n",
    "            current_section = 'columns'\n",
    "            cols = line.split(':')[1].strip()\n",
    "            for col_part in cols.split(','):\n",
    "                col_part = col_part.strip()\n",
    "                if col_part:\n",
    "                    if '(' in col_part and ')' in col_part:\n",
    "                        agg = col_part.split('(')[0].strip().upper()\n",
    "                        col = col_part.split('(')[1].split(')')[0].strip()\n",
    "                        components['columns'].append((agg, col))\n",
    "                    else:\n",
    "                        components['columns'].append((None, col_part))\n",
    "        elif line.startswith('Filters:'):\n",
    "            current_section = 'filters'\n",
    "            filters = line.split(':')[1].strip()\n",
    "            components['filters'] = [f.strip() for f in filters.split(',') if f.strip()]\n",
    "        elif line.startswith('Joins:'):\n",
    "            current_section = 'joins'\n",
    "            joins = line.split(':')[1].strip()\n",
    "            components['joins'] = [j.strip() for j in joins.split(',') if j.strip()]\n",
    "        elif line.startswith('GroupBy:'):\n",
    "            current_section = 'group_by'\n",
    "            group_bys = line.split(':')[1].strip()\n",
    "            components['group_by'] = [g.strip() for g in group_bys.split(',') if g.strip()]\n",
    "        elif current_section:\n",
    "            # Handle multi-line sections\n",
    "            if current_section == 'tables':\n",
    "                components['tables'].extend([t.strip() for t in line.split(',') if t.strip()])\n",
    "            elif current_section == 'columns':\n",
    "                for col_part in line.split(','):\n",
    "                    col_part = col_part.strip()\n",
    "                    if col_part:\n",
    "                        if '(' in col_part and ')' in col_part:\n",
    "                            agg = col_part.split('(')[0].strip().upper()\n",
    "                            col = col_part.split('(')[1].split(')')[0].strip()\n",
    "                            components['columns'].append((agg, col))\n",
    "                        else:\n",
    "                            components['columns'].append((None, col_part))\n",
    "            elif current_section == 'filters':\n",
    "                components['filters'].extend([f.strip() for f in line.split(',') if f.strip()])\n",
    "            elif current_section == 'joins':\n",
    "                components['joins'].extend([j.strip() for j in line.split(',') if j.strip()])\n",
    "            elif current_section == 'group_by':\n",
    "                components['group_by'].extend([g.strip() for g in line.split(',') if g.strip()])\n",
    "    \n",
    "    # Ensure schema is included in table names\n",
    "    components['tables'] = [f\"SPURCHASE.{t.split('.')[-1]}\" if '.' not in t else t for t in components['tables']]\n",
    "    \n",
    "    # Ensure grouping columns are included in SELECT - CORRECTED VERSION\n",
    "    for group_col in components['group_by']:\n",
    "        # Check if this exact (None, group_col) pair exists\n",
    "        col_exists = any(col == (None, group_col) for col in components['columns'])\n",
    "        # Check if group_col appears in any non-aggregated column reference\n",
    "        col_part_of_ref = any(group_col in col[1] for col in components['columns'] if col[0] is None)\n",
    "        \n",
    "        if not col_exists and not col_part_of_ref:\n",
    "            components['columns'].append((None, group_col))\n",
    "    \n",
    "    return components\n",
    "\n",
    "def parse_sparql_results(xml_response: str) -> List[Dict]:\n",
    "    \"\"\"Parse SPARQL XML results into a list of dictionaries\"\"\"\n",
    "    try:\n",
    "        root = ET.fromstring(xml_response)\n",
    "        results = []\n",
    "        \n",
    "        for result in root.findall('.//{http://www.w3.org/2005/sparql-results#}result'):\n",
    "            row = {}\n",
    "            for binding in result:\n",
    "                var_name = binding.attrib['name']\n",
    "                value = binding[0]  # uri or literal\n",
    "                if value.tag.endswith('uri'):\n",
    "                    row[var_name] = value.text\n",
    "                elif value.tag.endswith('literal'):\n",
    "                    row[var_name] = value.text\n",
    "            results.append(row)\n",
    "        return results\n",
    "    except ET.ParseError as e:\n",
    "        print(f\"Error parsing XML: {e}\")\n",
    "        return []\n",
    "    \n",
    "def generate_sql(components: Dict) -> str:\n",
    "    \"\"\"Generate clean SQL query from the analyzed components\"\"\"\n",
    "    # Validate components\n",
    "    if not components[\"tables\"]:\n",
    "        raise ValueError(\"No tables identified for SQL generation\")\n",
    "    \n",
    "    # Clean all components first\n",
    "    def clean_component(component):\n",
    "        return component.replace('[', '').replace(']', '').strip()\n",
    "    \n",
    "    # Build SELECT clause - ensure GROUP BY columns are included\n",
    "    select_parts = []\n",
    "    \n",
    "    # First add all GROUP BY columns to SELECT if they're not already there\n",
    "    # for group_col in components.get(\"group_by\", []):\n",
    "    #     group_col = clean_component(group_col)\n",
    "    #     if not any(col[1] == group_col for col in components[\"columns\"] if col[0] is None):\n",
    "    #         select_parts.append(group_col)\n",
    "    \n",
    "    # Then add the requested columns\n",
    "    for agg, col in components[\"columns\"]:\n",
    "        col = clean_component(col)\n",
    "        if not col:\n",
    "            continue\n",
    "        if agg:\n",
    "            select_parts.append(f\"{agg}({col})\")\n",
    "        else:\n",
    "            if col not in select_parts:  # Don't add duplicates\n",
    "                select_parts.append(col)\n",
    "    \n",
    "    if not select_parts:  # Default to all columns if none specified\n",
    "        select_parts.append(\"*\")\n",
    "\n",
    "    select_clause = \", \".join(select_parts[1:])\n",
    "    print(\"SELECT BEFORE \"+select_clause)\n",
    "    print(select_parts)\n",
    "    \n",
    "    # Build FROM clause\n",
    "    from_table = clean_component(components[\"tables\"][0])\n",
    "    from_clause = from_table\n",
    "    \n",
    "    # Add joins only if they exist and are not empty\n",
    "    join_clauses = []\n",
    "    for join in components.get(\"joins\", []):\n",
    "        clean_join = clean_component(join)\n",
    "        if clean_join and clean_join != 'INNER JOIN':\n",
    "            join_clauses.append(f\"INNER JOIN SPURCHASE.LFA1 ON {clean_join}\")\n",
    "    print(\"INNER JOIN \"+clean_join)\n",
    "    \n",
    "    # Build WHERE clause\n",
    "    where_clauses = []\n",
    "    for filter_cond in components.get(\"filters\", []):\n",
    "        clean_filter = clean_component(filter_cond)\n",
    "        if clean_filter:\n",
    "            where_clauses.append(clean_filter)\n",
    "    \n",
    "    where_clause = \" AND \".join(where_clauses) if where_clauses else \"\"\n",
    "    where_clause = where_clause.replace(\",\", \" AND\")\n",
    "    print(\"WHERE CLAUSE \"+where_clause)\n",
    "    \n",
    "    # Build GROUP BY clause\n",
    "    group_by_columns = [clean_component(g) for g in components.get(\"group_by\", []) if clean_component(g)]\n",
    "    group_by_clause = \", \".join(group_by_columns) if group_by_columns else \"\"\n",
    "    \n",
    "    # Construct the SQL\n",
    "    sql = f\"SELECT {select_clause} FROM {from_clause}\"\n",
    "    \n",
    "    if join_clauses:\n",
    "        sql += \" \" + \" \".join(join_clauses)\n",
    "    \n",
    "    if where_clause:\n",
    "        sql += f\" WHERE {where_clause}\"\n",
    "    \n",
    "    if group_by_clause:\n",
    "        sql += f\" GROUP BY {group_by_clause}\"\n",
    "    \n",
    "    # Final formatting\n",
    "    sql = sql.strip()\n",
    "    if not sql.endswith(';'):\n",
    "        sql += ';'\n",
    "    \n",
    "    return sql\n",
    "\n",
    "def execute_sql(sql_query: str, conn) -> pd.DataFrame:\n",
    "    \"\"\"Execute the generated SQL query and return results\"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(sql_query)\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "        rows = cursor.fetchall()\n",
    "        return pd.DataFrame(rows, columns=columns)\n",
    "    except Exception as e:\n",
    "        print(f\"Error executing SQL query: {e}\")\n",
    "        return pd.DataFrame()\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "\n",
    "def process_question(question: str, conn, anthropic) -> pd.DataFrame:\n",
    "    \"\"\"Main function to process a user question with better error handling\"\"\"\n",
    "    try:\n",
    "        # Step 1: Extract relevant metadata using SPARQL\n",
    "        metadata = extract_metadata(conn)\n",
    "        \n",
    "        if not metadata:\n",
    "            return \"Could not retrieve database metadata.\"\n",
    "        \n",
    "        # Step 2: Analyze the metadata and question\n",
    "        components = analyze_metadata(metadata, question, anthropic)\n",
    "        \n",
    "        # Step 3: Generate SQL query\n",
    "        sql_query = generate_sql(components)\n",
    "    \n",
    "        # Step 4: Execute SQL\n",
    "        sql_result = execute_sql(sql_query, conn)\n",
    "        \n",
    "        return sql_result\n",
    "    except Exception as e:\n",
    "        return f\"Error processing question: {str(e)}\"\n",
    "\n",
    "rdf_context = process_question(question, conn, anthropic_model)\n",
    "print(\"\\nKnowledge Graph Context Retrieved:\")\n",
    "print(rdf_context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e50f28",
   "metadata": {},
   "source": [
    "### Step 7: Generate Final Answer\n",
    "By leveraging both sources simultaneously:\n",
    "- The vector engine provides contextual richness and flexibility, helping the model understand the user's intent in a nuanced way.\n",
    "- The knowledge graph ensures precision, factual consistency, and traceability by grounding answers in authoritative structured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e4218d",
   "metadata": {},
   "outputs": [],
   "source": [
    "hybrid_prompt_template = PromptTemplate(\n",
    "    template=\"\"\"\n",
    "        Context: You are tasked with helping retrieve and summarize supplier information.\n",
    "\n",
    "        Available information:\n",
    "        - Unstructured document chunks ({vector_context}).\n",
    "        - Structured knowledge graph results ({kg_context}).\n",
    "\n",
    "        User Question:\n",
    "        {question}\n",
    "\n",
    "        Instructions:\n",
    "        - Only suppliers that appear in both the unstructured document chunks Structured knowledge graph results may be included in the final answer.\n",
    "        - Use structured knowledge graph results to filter the suppliers already found in the unstructured document chunks.\n",
    "        - Do not infer or assume facts ‚Äî all conclusions must be backed by knowledge graph validation.\n",
    "        - Optionally, include supporting information from unstructured data *if* it aligns with the knowledge graph result.\n",
    "        - Prefer clarity and structured presentation (use lists or bullets). Optionally, return structured JSON if many suppliers are involved.\n",
    "\n",
    "        Return:\n",
    "        - A clean, human-readable summary limited to validated suppliers only.\n",
    "        - Ensure all risk scores are grounded in validated graph entries.\n",
    "\n",
    "        \"\"\",\n",
    "    input_variables=[\"vector_context\", \"kg_context\", \"question\"]\n",
    ")\n",
    "# Generate final answer using LLM\n",
    "hybrid_answer_llm_chain = hybrid_prompt_template | gpt_model\n",
    "hybrid_answer = hybrid_answer_llm_chain.invoke({\n",
    "    \"vector_context\": vector_context,\n",
    "    \"kg_context\": rdf_context,\n",
    "    \"question\": question\n",
    "}).content.strip()\n",
    "\n",
    "print(\"\\n=== Hybrid RAG Answer ===\\n\")\n",
    "print(hybrid_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
