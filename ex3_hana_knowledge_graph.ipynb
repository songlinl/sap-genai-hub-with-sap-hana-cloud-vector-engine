{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "In this section, we will focus on building **Knowledge Graphs (KGs)** from **tabular data**. Specifically, we will use tables **S013** and **LFA1** from the **SPURCHASE** schema to construct an **ontology** that captures the semantic relationships between these tables. The ontology will define key aspects such as table relationships, join conditions, and relevant columns. This semantic layer serves as the foundation for generating knowledge graphs, enabling more meaningful data integration and querying. Once the ontology is created, we will import it into **SAP HANA Cloud** to generate the Knowledge Graphs. The import can be done either via a local file or through supported cloud storage options. \n",
    "\n",
    "With the ontology and knowledge graph in place, we aim to **answer complex business or analytical questions** that are grounded in underlying **tabular data** by leveraging the ontologies created and stored within the **SAP HANA Cloud database**. These ontologies provide a **semantic layer** that describes the structure, meaning, and relationships of the data in a machine-readable format, enabling more intelligent and context-aware data access. By using this semantic model‚Äîbuilt from metadata such as table relationships, column meanings, and domain-specific concepts‚Äîwe can translate user queries or application logic into meaningful insights, even when the data resides in multiple interrelated tables. This approach enhances data **discovery, reasoning, and integration**, and supports more intuitive querying through graph-based models rather than complex joins or manual interpretation of raw table structures.\n",
    "\n",
    "### About the dataset\n",
    "This dataset is a **simulated version of vendor data** from **SAP S/4HANA**, created for demonstration purposes. It includes **vendor evaluation records (S013)** and **supplier master data (LFA1)**, which are essential for assessing vendor performance, identifying potential supply chain risks, and supporting data-driven procurement decisions.\n",
    "\n",
    "For the demo calculation of risk score on table S013, we used the following weighting key (sums to 1.00):\n",
    "|Criterion (column)\t|Description\t|Weight |\n",
    "|-------------------|---------------|-------|\n",
    "|**PWMT1**\t|Quantity-reliability points\t|0.15|\n",
    "|**PWTT1**\t|On-time-delivery points\t|0.15|\n",
    "|**PWEV1**\t|Compliance with shipping instructions\t|0.10|\n",
    "|**PWWE1**\t|Service-quality points\t|0.15|\n",
    "|**PWFR1**\t|Service timeliness\t|0.10|\n",
    "|**PWQA1**\t|Quality-audit points\t|0.10|\n",
    "|**LAVI1**\t|Shipping-notification performance\t|0.15|\n",
    "|**ALAV1**\t|Variance from shipping notification\t|0.10|\n",
    "|**Total**\t|‚Äî\t|1.00|\n",
    "\n",
    "- **VENDOR_SCORE** = Œ£( score √ó weight ) / 1.00  \n",
    "- **RISK_SCORE**   = 100 ‚Äì VENDOR_SCORE\n",
    "\n",
    "> **Important**: \n",
    "> - These weights are just an example. In a real SAP system they must mirror the weighting key defined in SPRO ‚ñ∏ Materials Management ‚ñ∏ Purchasing ‚ñ∏ Vendor Evaluation ‚ñ∏ Define Weighting Keys.\n",
    "> - If your system uses the standard key 000 (equal importance) you would set each weight to 0.125 (12.5 %) or simply average the eight criteria.\n",
    "> - Management dashboards often base the overall vendor rating on the ‚Äú‚Ä¶2‚Äù values to avoid wild month-to-month swings.\n",
    "> - If you load true historical S013 data (several periods) into HANA, switching the formula to the ‚Äú‚Ä¶2‚Äù fields will give exactly the same logic SAP uses in the standard report.\n",
    "\n",
    "### Retrival Augmented Generation (RAG) using SAP HANA Knowledge Graph Engine\n",
    "The RAG use case process consists of steps to be completed as seen in the graphic below. The process involves querying the Knowledge Graph, retrieving relevant data, and generating insights based on the retrieved information. \n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "> ![title](./images/rag_kg.png)\n",
    "\n",
    "<br> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Knowledge Graph Creation Process\n",
    "1. **Extract Metadata from Tabular Data**  \n",
    "Retrieve schema information (e.g., table names, columns, data types, and relationships) from SAP HANA Cloud tables.\n",
    "\n",
    "2. **Define Ontology**  \n",
    "Create an ontology that semantically models the entities, attributes, and relationships found in the metadata (e.g., using RDF/OWL formats).\n",
    "\n",
    "3. **Construct RDF Graph**  \n",
    "Convert the metadata into RDF triples using Python libraries (e.g., rdflib) to represent the ontology as a machine-readable graph.\n",
    "\n",
    "4. **Serialize to Turtle Format**  \n",
    "Serialize the RDF graph into a .ttl (Turtle) file, a compact and readable format for RDF data.\n",
    "\n",
    "5. **Import into SAP HANA Cloud**  \n",
    "Load the Turtle file into the SAP HANA Cloud database using appropriate tools or scripts to create the persistent Knowledge Graph.\n",
    "\n",
    "6. **Validate the Knowledge Graph**  \n",
    "Run SPARQL queries to ensure the structure accurately represents the original data and semantic relationships.\n",
    "\n",
    "### Knowledge Graph Retrieval Process\n",
    "1. **Extract Relevant Metadata using SPARQL**  \n",
    "Query the knowledge graph with SPARQL to retrieve semantic information such as table relationships, column descriptions, and data types relevant to the question.\n",
    "\n",
    "2. **Analyze the Metadata and Natural Language Question**  \n",
    "Understand the user query by parsing the intent and identifying key entities and attributes. Match these elements to the retrieved metadata.\n",
    "\n",
    "3. **Generate SQL Query**  \n",
    "Based on the analysis, formulate a context-aware SQL query that aligns with the user's intent and the structure of the underlying tabular data.\n",
    "\n",
    "4. **Execute the SQL Query**  \n",
    "Run the generated SQL against the SAP HANA Cloud database to retrieve the actual data.\n",
    "\n",
    "5. **Generate and Return the Final Response**  \n",
    "Convert the query result into a human-readable answer or structured output, optionally using a language model to enhance clarity and context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup and configuration\n",
    "\n",
    "The following python modelues are to be installed during this hands-on introduction.\n",
    "\n",
    "**rdflib**  \n",
    "RDFLib is a pure Python package for working with RDF. RDFLib contains most things you need to work with RDF, including:\n",
    "- parsers and serializers for RDF/XML, N3, NTriples, N-Quads, Turtle, TriX, Trig and JSON-LD\n",
    "- a Graph interface which can be backed by any one of a number of Store implementations\n",
    "- store implementations for in-memory, persistent on disk (Berkeley DB) and remote SPARQL endpoints\n",
    "- a SPARQL 1.1 implementation - supporting SPARQL 1.1 Queries and Update statements\n",
    "- SPARQL function extension mechanisms\n",
    "\n",
    "For more information, please see https://pypi.org/project/rdflib/  \n",
    "\n",
    "**hdbcli**  \n",
    "The Python Database API Specification v2.0 (PEP 249) defines a set of methods that provides a consistent database interface independent of the actual database being used. The Python extension module for SAP HANA implements PEP 249. Once you install the module, you can access and change the information in SAP HANA databases from Python.\n",
    "\n",
    "For more information, please see https://pypi.org/project/hdbcli/  \n",
    "\n",
    "#### Install Python Packages  \n",
    "Run the following package installations. **pip** is the package installer for Python. You can use pip to install packages from the Python Package Index and other indexes. \n",
    "\n",
    "**Note:** Jupyter Notebook kernel restart required after package installation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install rdflib --break-system-packages\n",
    "%pip install hdbcli --break-system-packages\n",
    "%pip install generative-ai-hub-sdk[all] --break-system-packages\n",
    "%pip install langchain_core --break-system-packages\n",
    "%pip install pandas --break-system-packages\n",
    "%pip install xml.etree --break-system-packages\n",
    "%pip install python-dotenv --break-system-packages\n",
    "# kernel restart required!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Knowledge Graph\n",
    "Here are the typical steps to create a knowledge graph from tabular data in SAP HANA Cloud:\n",
    "\n",
    "1. **Extract Metadata from Tables**\n",
    "    - Extract metadata such as table and column names, data types, relationships (foreign keys), and constraints.\n",
    "    - Optionally, gather descriptions and aggregation rules if available.\n",
    "\n",
    "2. **Define the Ontology or Semantic Model**\n",
    "    - Create an ontology that represents the entities, attributes, and relationships based on the extracted metadata.\n",
    "    - Define classes (for tables/entities), properties (for columns/attributes), and relationships (joins or foreign keys).\n",
    "    - This semantic model forms the basis of the knowledge graph.\n",
    "\n",
    "3. **Convert Tabular Data into RDF Triples**\n",
    "    - Map rows and columns to RDF triples of the form (subject, predicate, object).\n",
    "    - Subjects typically represent entities (e.g., vendor records).\n",
    "    - Predicates represent attributes or relationships.\n",
    "    - Objects are values or related entities.\n",
    "\n",
    "4. **Load RDF Triples into SAP HANA Cloud**\n",
    "    - Create tables or graph workspace structures in SAP HANA Cloud to hold RDF triples or graph data.\n",
    "    - Import the RDF triples into these structures (using RDFLib and hdbcli).\n",
    "\n",
    "#### Initialize the RDF graph and add table metadata into the graph\n",
    "This code snippet demonstrates how to initialize an RDF graph using a Python library such as RDFLib, and how to construct the graph by extracting metadata from database tables, such as table names, column names, data types, and relationships (e.g., foreign key constraints). This metadata is then translated into RDF triples, capturing the structure and semantics of the underlying tabular data. The resulting RDF graph serves as the foundation for building ontologies or knowledge graphs, enabling semantic reasoning and enhanced data integration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdflib import Graph, Literal, Namespace, RDF, RDFS, XSD\n",
    "\n",
    "# Define custom namespaces for our RDF graph\n",
    "ns = Namespace(\"http://supplychain_database.org/spurchase/\")  # Namespace for supply chain data\n",
    "db = Namespace(\"http://supplychain_database.org/database/\")  # Namespace for database schema\n",
    "\n",
    "# Create an empty RDF graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind namespace prefixes for cleaner serialization\n",
    "g.bind(\"spurchase\", ns)  # Associates \"spurchase\" prefix with our namespace\n",
    "g.bind(\"db\", db)       # Associates \"db\" prefix with database namespace\n",
    "\n",
    "# Define table resources\n",
    "s013 = ns.S013      # Resource for vendor evaluation table\n",
    "lfa1 = ns.LFA1  # Resource for vender metadata table\n",
    "\n",
    "# Add metadata for S013 table\n",
    "g.add((s013, RDF.type, db.Table))            # Set type as Table\n",
    "g.add((s013, RDFS.label, Literal(\"VENDOR EVALUATION\")))  # Human-readable label\n",
    "g.add((s013, db.tableName, Literal(\"S013\"))) # Actual table name in database\n",
    "\n",
    "# Add metadata for LFA1 table\n",
    "g.add((lfa1, RDF.type, db.Table))            # Set type as Table\n",
    "g.add((lfa1, RDFS.label, Literal(\"VENDOR MASTER DATA\")))  # Human-readable label\n",
    "g.add((lfa1, db.tableName, Literal(\"LFA1\"))) # Actual table name in database\n",
    "\n",
    "print(\"Your knowledge graph is initialized!‚úÖ\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define columns and metadata for Table S013\n",
    "The following code snippet constructs an RDF graph by extracting metadata from the database table S013, which is typically used for storing vendor evaluation data. The extracted metadata includes column names, column descriptions, applicable aggregation methods, and relationships to other tables, such as those defined by foreign key constraints. This metadata is then transformed into RDF triples, where each piece of information is represented as a semantic statement. By doing so, the graph captures not only the structure but also the contextual meaning of the data within S013, forming an essential component of a broader knowledge graph for enhanced data integration, reasoning, and analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns and metadata for S013 table\n",
    "s013_columns = {\n",
    "    # Client column metadata\n",
    "    ns.MANDT: {\n",
    "        \"label\": \"Client\",\n",
    "        \"isKey\": True,  # Mark as primary key\n",
    "        \"description\": \"Client identifier\"\n",
    "    },\n",
    "    # Vendor ID column metadata (foreign key)\n",
    "    ns.LIFNR: {\n",
    "        \"label\": \"Vendor ID\",\n",
    "        \"isKey\": True,  # Part of composite key\n",
    "        \"foreignKey\": ns.LIFNR,  # References LFA1.LIFNR\n",
    "        \"groupBy\": True,  # Can be used for grouping\n",
    "        \"description\": \"Account number of vendor\"\n",
    "    },\n",
    "    # Material Number column metadata\n",
    "    ns.MATNR: {\n",
    "        \"label\": \"Material Number\",\n",
    "        \"isKey\": True,  # Part of composite key\n",
    "        \"groupBy\": True,  # Can be used for grouping\n",
    "        \"description\": \"Material Number\"\n",
    "    },\n",
    "    # Month column metadata\n",
    "    ns.SPMON: {\n",
    "        \"label\": \"Month\",\n",
    "        \"groupBy\": True,  # Can be used for grouping\n",
    "        \"description\": \"Month to analyze\"\n",
    "    },\n",
    "    # Purchasing Organization column metadata (foreign key)\n",
    "    ns.EKORG: {\n",
    "        \"label\": \"Purchasing Organization\",\n",
    "        \"aggregation\": db.COUNT,  # Can be used with COUNT function\n",
    "        \"description\": \"Purchasing organization\"\n",
    "    },\n",
    "    # Quantity Reliability column metadata\n",
    "    ns.PWMT1: {\n",
    "        \"label\": \"Quantity Reliability\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score for quantity reliability\"\n",
    "    },\n",
    "    # On-time Delivery column metadata\n",
    "    ns.PWTT1: {\n",
    "        \"label\": \"On-time Delivery\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score for on-time delivery performance\"\n",
    "    },\n",
    "    # Shipping Compliance column metadata\n",
    "    ns.PWEV1: {\n",
    "        \"label\": \"Shipping Compliance\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score for compliance with shipping instructions\",\n",
    "    },\n",
    "    # Service Quality column metadata\n",
    "    ns.PWWE1: {\n",
    "        \"label\": \"Service Quality\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score for service quality\",\n",
    "    },\n",
    "    # Service Timeliness column metadata\n",
    "    ns.PWFR1: {\n",
    "        \"label\": \"Service Timeliness\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score for service timeliness\",\n",
    "    },\n",
    "    # Quality Audit column metadata\n",
    "    ns.PWQA1: {\n",
    "        \"label\": \"Quality Audit\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score for quality audit\",\n",
    "    },\n",
    "    # Shipping Notification column metadata\n",
    "    ns.LAVI1: {\n",
    "        \"label\": \"Shipping Notification\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score for shipping notification\",\n",
    "    },\n",
    "    # Variance from Shipping Notification column metadata\n",
    "    ns.ALAV1: {\n",
    "        \"label\": \"Variance from Shipping Notification\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Points score for variance from shipping notification\",\n",
    "    },\n",
    "    # Vendor Reliability column metadata\n",
    "    ns.RELIA: {\n",
    "        \"label\": \"Vendor Reliability\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Vendor reliability score\",\n",
    "    },\n",
    "    # Vendor Risk column metadata\n",
    "    ns.RISK1: {\n",
    "        \"label\": \"Vendor Risk\",\n",
    "        \"aggregation\": db.AVG,  # Can be used with AVG function\n",
    "        \"description\": \"Vendor risk score\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add all S013 columns to the graph\n",
    "for col, meta in s013_columns.items():\n",
    "    # Basic column metadata\n",
    "    g.add((col, RDF.type, db.Column))  # Set type as Column\n",
    "    g.add((col, RDFS.label, Literal(meta[\"label\"])))  # Human-readable label\n",
    "    g.add((col, db.columnName, Literal(col.split(\"/\")[-1])))  # Extract column name from URI\n",
    "    g.add((col, db.description, Literal(meta[\"description\"])))  # Description\n",
    "\n",
    "    # Conditional metadata additions\n",
    "    if meta.get(\"isKey\"):\n",
    "        g.add((col, db.isPrimaryKey, Literal(True)))  # Mark as primary key\n",
    "    if meta.get(\"groupBy\"):\n",
    "        g.add((col, db.groupBy, Literal(True)))  # Mark as groupable\n",
    "    if meta.get(\"aggregation\"):\n",
    "        g.add((col, db.aggregationFunction, meta[\"aggregation\"]))  # Add aggregation function\n",
    "    # if meta.get(\"filter\"):\n",
    "    #    g.add((col, db.filterFunction, Literal(meta[\"filter\"])))  # Add filter function\n",
    "    if meta.get(\"foreignKey\"):\n",
    "        g.add((col, db.foreignKey, meta[\"foreignKey\"]))  # Add foreign key reference\n",
    "\n",
    "print(\"‚úÖTable S013 has been added to your knowledge graph.\")\n",
    "print(f\"üëçThere are {len(g)} triples in your knowledge graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define columns and metadata for Table LFA1\n",
    "The following code snippet constructs an RDF graph by programmatically extracting metadata from the database table LFA1, which typically contains vendor master data. This metadata includes details such as column names, column descriptions, data types, potential aggregation rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns and metadata for LFA1 table\n",
    "lfa1_columns = {\n",
    "    # Vendor ID column metadata (foreign key)\n",
    "    ns.LIFNR: {\n",
    "        \"label\": \"Vendor ID\",\n",
    "        \"isKey\": True,  # Primary key\n",
    "        \"description\": \"Account number of vendor\"\n",
    "    },\n",
    "    # Vendor name column metadata\n",
    "    ns.NAME1: {\n",
    "        \"label\": \"Vendor Name\",\n",
    "        \"description\": \"Full name of vendors\"\n",
    "    },\n",
    "    # Country Key column metadata\n",
    "    ns.LAND1: {\n",
    "        \"label\": \"Country Key\",\n",
    "        \"description\": \"Country code\"\n",
    "    },\n",
    "    # City column metadata\n",
    "    ns.ORT01: {\n",
    "        \"label\": \"City\",\n",
    "        \"description\": \"City\"\n",
    "    },\n",
    "    # Address column metadata\n",
    "    ns.STRAS: {\n",
    "        \"label\": \"Vendor Address\",\n",
    "        \"description\": \"House number and street\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Add all LFA1 columns to the graph\n",
    "for col, meta in lfa1_columns.items():\n",
    "    # Basic column metadata\n",
    "    g.add((col, RDF.type, db.Column))  # Set type as Column\n",
    "    g.add((col, RDFS.label, Literal(meta[\"label\"])))  # Human-readable label\n",
    "    g.add((col, db.columnName, Literal(col.split(\"/\")[-1])))  # Extract column name from URI\n",
    "    g.add((col, db.description, Literal(meta[\"description\"])))  # Description\n",
    "    # Conditional metadata additions\n",
    "    if meta.get(\"isKey\"):\n",
    "        g.add((col, db.isPrimaryKey, Literal(True)))  # Mark as primary key\n",
    "\n",
    "print(\"‚úÖTable LFA1 has been added to your knowledge graph.\")\n",
    "print(f\"üëçThere are {len(g)} triples in your knowledge graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define relationships between tables\n",
    "The following code snippet constructs an RDF graph by explicitly defining the semantic relationships between the two database tables, S013 and LFA1. It establishes how these tables are linked‚Äîtypically through foreign keys or logical associations‚Äîand represents these connections using RDF triples. Each triple encodes a subject-predicate-object relationship, allowing the graph to capture the meaning and structure of the data. This forms the basis for integrating relational data into a knowledge graph, enabling advanced querying, reasoning, and data interoperability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define relationships between tables\n",
    "g.add((s013, db.relatedTo, lfa1))  # General relationship between tables\n",
    "\n",
    "# Explicit foreign key relationship\n",
    "g.add((ns.LIFNR, db.foreignKey, ns.LIFNR))  # S013.LIFNR ‚Üí LFA1.LIFNR\n",
    "\n",
    "# Join condition for the relationship\n",
    "g.add((ns.LIFNR, db.joinCondition, Literal(\"S013.MANDT = LFA1.MANDT AND S013.LIFNR = LFA1.LIFNR\")))\n",
    "\n",
    "print(\"‚úÖTable relations have been added to your knowledge graph.\")\n",
    "print(f\"üëçThere are {len(g)} triples in your knowledge graph.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Serialize the graph in Turtle (Terse RDF Triple Language) format\n",
    "\n",
    "The Turtle data format is a way of representing data using the RDF. It's a form of serializing RDF data in a human-readable and easy-to-write format. Turtle is defined by the W3C and uses a syntax like the N-Triples format, with added support for prefixes and shorthand notations to make it more concise and readable. Turtle is commonly used for writing and sharing RDF data on the web."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serialize the graph to Turtle format\n",
    "graph_string = g.serialize(format=\"turtle\")\n",
    "\n",
    "# Write the Turtle string to a file\n",
    "with open('./spurchase_tabular.ttl', 'w') as file:\n",
    "    file.write(graph_string)\n",
    "\n",
    "print(\"‚úÖTurtle file has been generated locally.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAP HANA Cloud Knowledge Graph engine\n",
    "\n",
    "The SAP HANA Cloud knowledge graph engine is an integral part of SAP HANA Cloud. It is an addition to the SAP HANA Cloud mult model engines. It is designed for processing large-scale graph data and executing complex graph queries with high efficiency using knowledge graphs. SAP HANA Cloud knowledge graph engine enables organizations to gain insights from their graph datasets, discover patterns, perform advanced graph analytics and unlock the value of interconnected data.\n",
    "\n",
    "The SAP HANA Cloud knowledge graph engine enables working with knowledge graphs. It is built using World Wide Web Consortium (W3C) web standards specifications of graph data called RDF (containing triples with the format subject-predicate-object) and its associated W3C web standards query language called SPARQL. Knowledge graphs are stored in a triple store.\n",
    "\n",
    "#### Connect to SAP HANA Cloud database\n",
    "\n",
    "The provided Python script imports database connection modules and initiates a connection to a SAP HANA Cloud instance using the `dbapi` module. The user is prompted to enter their username and password, which are then used to establish a secure connection to the SAP HANA Cloud database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up HANA Cloud Connection to import the ttl file\n",
    "from hdbcli import dbapi\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # take environment variables from .env.\n",
    "\n",
    "# Get the HANA Cloud username from environment variables\n",
    "HANA_USER = os.getenv('HANA_VECTOR_USER')\n",
    "# Get the HANA Cloud password from environment variables\n",
    "HANA_PASS = os.getenv('HANA_VECTOR_PASS')\n",
    "# Get the HANA Cloud host from environment variables\n",
    "HANA_HOST = os.getenv('HANA_VECTOR_HOST')\n",
    "\n",
    "# Establish connection to SAP HANA Cloud database\n",
    "conn = dbapi.connect(\n",
    "    user = HANA_USER,\n",
    "    password = HANA_PASS,\n",
    "    address = HANA_HOST,\n",
    "    port = 443,\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "print(\"‚úÖHANA Cloud connection is established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import the Turtle file into SAP HANA Cloud\n",
    "Finally, the Turtle file‚Äîcontaining structured RDF triples that define the semantic relationships between entities‚Äîwill be imported into the SAP HANA Cloud database, where it will be stored and represented as a knowledge graph. This process enables the underlying data to be queried and analyzed in a graph-based format, allowing for advanced semantic reasoning, relationship exploration, and integration with SAP HANA‚Äôs native graph processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the ttl file into SAP HANA Cloud\n",
    "ttl_filename = \"./spurchase_tabular.ttl\"\n",
    "graphname = \"spurchase_graph_\" + HANA_USER\n",
    "try:\n",
    "    with open(ttl_filename, 'r') as ttlfp:\n",
    "        request_hdrs = ''\n",
    "        request_hdrs += 'rqx-load-protocol: true' + '\\r\\n'            # required header for upload protocol\n",
    "        request_hdrs += 'rqx-load-filename: ' + ttl_filename + '\\r\\n' # optional header\n",
    "        request_hdrs += 'rqx-load-graphname: ' + graphname + '\\r\\n'   # optional header to specify name of the graph, if not provided RDF data will be loaded to internal-default-graph\n",
    "        conn.cursor().callproc('SPARQL_EXECUTE', (ttlfp.read(), request_hdrs, '', None))\n",
    "\n",
    "    print(\"‚úÖSuccess! The RDF graph has been successfully ingested into SAP HANA Cloud as graph:\", graphname)\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"‚ùåError occurred while ingesting the graph:\", str(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validate the Knowledge Graph in SAP HANA Cloud\n",
    "We will perform queries on the ontologies stored in the SAP HANA Cloud database to explore and validate the semantic structures that have been generated from the underlying tabular data. These queries will help verify that the RDF triples or graph representations correctly model the relationships, hierarchies, and attributes as defined in the original database schema. By doing so, we can ensure that the **graph structure** not only faithfully mirrors the actual tabular data‚Äîsuch as tables, columns, and foreign key constraints‚Äîbut also captures the **intended semantic relationships** between different entities. This validation step is crucial to confirm the **consistency, completeness, and integrity** of the knowledge graph before it is used for advanced semantic analysis, data integration, or reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all imported ontologies\n",
    "validation_query = \"\"\"\n",
    "SELECT ?graph (COUNT(*) as ?num_triples)\n",
    "WHERE {\n",
    "    GRAPH ?graph { [?p []] }\n",
    "}\n",
    "GROUP BY ?graph\n",
    "ORDER BY ?graph\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SPARQL query\n",
    "try:\n",
    "    resp = cursor.callproc('SPARQL_EXECUTE', (\n",
    "        validation_query,\n",
    "        'Accept: application/sparql-results+csv',\n",
    "        '?',\n",
    "        None\n",
    "    ))\n",
    "    # metadata = resp[3]\n",
    "    # results = resp[2]\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Validation Query Results:\")\n",
    "    print(resp[2])\n",
    "    # print(\"Response Metadata:\", metadata)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"SPARQL_EXECUTE failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the following code to see all the triples from the imported knowledge graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the graph we imported\n",
    "\n",
    "validation_query = \"\"\"\n",
    "SELECT * WHERE {\n",
    "  GRAPH <\"\"\" + graphname + \"\"\"> {\n",
    "    ?s ?p ?o\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SPARQL query\n",
    "try:\n",
    "    resp = cursor.callproc('SPARQL_EXECUTE', (\n",
    "        validation_query,\n",
    "        'Accept: application/sparql-results+csv',\n",
    "        '?',\n",
    "        None\n",
    "    ))\n",
    "    # metadata = resp[3]\n",
    "    # results = resp[2]\n",
    "\n",
    "    # Print results\n",
    "    print(\"Validation Query Results:\")\n",
    "    print(resp[2])\n",
    "    # print(\"Response Metadata:\", metadata)\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"SPARQL_EXECUTE failed: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Graph Retrieval Chain\n",
    "\n",
    "#### Set Up AI Core Client\n",
    "Execute the configuration module below to enable access to SAP‚Äôs Generative AI foundation models. Running this code block will automatically handle the necessary setup, including authentication and environment configuration, to ensure seamless connectivity to the Generative AI services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ai_core_sdk.ai_core_v2_client import AICoreV2Client\n",
    "from gen_ai_hub.proxy.gen_ai_hub_proxy import GenAIHubProxyClient\n",
    "\n",
    "# Get the AI Core URL from environment variables\n",
    "URL = os.getenv('AICORE_AUTH_URL')\n",
    "# Get the AI Core client ID from environment variables\n",
    "CLIENT_ID = os.getenv('AICORE_CLIENT_ID')\n",
    "# Get the AI Core client secret from environment variables\n",
    "CLIENT_SECRET = os.getenv('AICORE_CLIENT_SECRET')\n",
    "# Get the AI Core client ID from environment variables\n",
    "RESOURCE_GROUP = os.getenv('AICORE_RESOURCE_GROUP')\n",
    "# Get the AI Core client secret from environment variables\n",
    "API_URL = os.getenv('AICORE_BASE_URL')\n",
    "\n",
    "# Set up the AICoreV2Client\n",
    "ai_core_client = AICoreV2Client(base_url=API_URL,\n",
    "                            auth_url=URL,\n",
    "                            client_id=CLIENT_ID,\n",
    "                            client_secret=CLIENT_SECRET,\n",
    "                            resource_group=RESOURCE_GROUP)\n",
    "\n",
    "# Initialize GenAIHub proxy client\n",
    "proxy_client = GenAIHubProxyClient(ai_core_client=ai_core_client)\n",
    "print(\"‚úÖAI Core Client connection is established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize the large language model\n",
    "The large language model (LLM) is initialized as an instance of ChatBedrock using the **anthropic--claude-3.5-sonnet** model. This instance serves as the conversational interface, enabling the generation of context-aware responses and facilitating interactions in a chat-like environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gen_ai_hub.proxy.langchain.amazon import ChatBedrock\n",
    "# Initialize the ChatBedrock client with the proxy client\n",
    "anthropic = ChatBedrock(\n",
    "    model_name=\"anthropic--claude-3.5-sonnet\",\n",
    "    proxy_client=proxy_client # Pass the proxy client to ChatBedrock\n",
    ")\n",
    "print(\"‚úÖLLM model connection is established successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Relevant Metadata using SPARQL\n",
    "This script executes a SPARQL query against a knowledge graph stored in SAP HANA Cloud, parses the XML response, and extracts the resulting RDF triples (subject, predicate, object) into a Python list called metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "# Execute SPARQL query to get all relevant triples\n",
    "sparql_query = \"\"\"\n",
    "SELECT * WHERE {\n",
    "  GRAPH <\"\"\" + graphname + \"\"\"> {\n",
    "    ?s ?p ?o\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "    \n",
    "try:\n",
    "    cursor = conn.cursor()\n",
    "    resp = cursor.callproc('SPARQL_EXECUTE', (sparql_query, 'Metadata headers describing Input and/or Output', '?', None))\n",
    "    \n",
    "    if resp and len(resp) >= 3 and resp[2]:\n",
    "        # Parse the XML response\n",
    "        xml_response = resp[2]\n",
    "        try:\n",
    "            root = ET.fromstring(xml_response)\n",
    "            results = []\n",
    "            \n",
    "            for result in root.findall('.//{http://www.w3.org/2005/sparql-results#}result'):\n",
    "                row = {}\n",
    "                for binding in result:\n",
    "                    var_name = binding.attrib['name']\n",
    "                    value = binding[0]  # uri or literal\n",
    "                    if value.tag.endswith('uri'):\n",
    "                        row[var_name] = value.text\n",
    "                    elif value.tag.endswith('literal'):\n",
    "                        row[var_name] = value.text\n",
    "                results.append(row)\n",
    "        except ET.ParseError as e:\n",
    "            print(f\"Error parsing XML: {e}\")\n",
    "        # results = parse_sparql_results(xml_response)\n",
    "        \n",
    "        # Convert to our standard format\n",
    "        metadata = []\n",
    "        for row in results:\n",
    "            metadata.append({\n",
    "                's': row.get('s', ''),\n",
    "                'p': row.get('p', ''),\n",
    "                'o': row.get('o', '')\n",
    "            })\n",
    "\n",
    "    print(f\"‚úÖSPARQL query executed successfully! Retrieved {len(metadata)} triples.\")\n",
    "\n",
    "    # Print the metadata\n",
    "    for item in metadata:\n",
    "        print(item)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error executing SPARQL query: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyze the Metadata and Natural Language Question\n",
    "This code snippet is designed to analyze a natural language question by leveraging metadata from a knowledge graph and using a large language model (LLM) (via LangChain and Anthropic) to extract relevant SQL components (tables, columns, filters, joins, etc.).\n",
    "\n",
    "Some other questions you can ask: \n",
    "1. What is the risk score for purchasing material 'MAT0151' from vendor '1011'?\n",
    "2. Find the vendors' name whose risk score is higher than 34 for the material MAT0151."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "# Analyze the metadata to identify tables, columns, and relationships\n",
    "# Convert metadata to a format the LLM can understand\n",
    "metadata_str = \"\\n\".join([f\"{item['s']} {item['p']} {item['o']}\" for item in metadata])\n",
    "question = \"Find the vendors's name whose risk score is higher than 34 for the material MAT0151.\"\n",
    "\n",
    "prompt_template = \"\"\"Given the following RDF metadata about database tables and columns, analyze the user's question and identify:\n",
    "1. The main table(s) involved with their schema (SPURCHASE)\n",
    "2. The columns needed (including any aggregation functions)\n",
    "3. Any filters or conditions\n",
    "4. Any joins required\n",
    "\n",
    "Important Rules:\n",
    "- Always include the schema name (SPURCHASE) before table names\n",
    "- When using GROUP BY, include the grouping columns in SELECT\n",
    "- Never include any explanatory text in the SQL output\n",
    "- For country codes like Germany, use 'DE' in filters\n",
    "\n",
    "For each column, include:\n",
    "- The column name (prefix with table alias if needed)\n",
    "- Any aggregation function (AVG, COUNT, etc.)\n",
    "- Any filter conditions\n",
    "- Whether it's a grouping column\n",
    "\n",
    "For tables, include:\n",
    "- The full table name with schema (e.g., SPURCHASE.S013)\n",
    "- Any relationships to other tables\n",
    "\n",
    "\n",
    "Metadata:\n",
    "{metadata}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Return your analysis in this exact format (without any additional explanations):\n",
    "Tables: [schema.table]\n",
    "Columns: [column names with aggregations like AVG(RISK1)]\n",
    "Filters: [filter conditions]\n",
    "Joins: [join conditions]\n",
    "GroupBy: [columns to group by]\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template).invoke({\n",
    "    \"metadata\": metadata_str,\n",
    "    \"question\": question\n",
    "})\n",
    "\n",
    "# We'll use the LLM to extract the key components\n",
    "analysis = anthropic.invoke(prompt)\n",
    "print(analysis.content)\n",
    "# return parse_analysis(analysis.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the LLM Response\n",
    "This code parses and structures the response from a Large Language Model (LLM), which analyzed a natural language question and metadata to return components for an SQL query (e.g., tables, columns, filters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a library to parse the analysis content\n",
    "components = {\n",
    "    \"tables\": [],\n",
    "    \"columns\": [],\n",
    "    \"filters\": [],\n",
    "    \"joins\": [],\n",
    "    \"group_by\": []\n",
    "}\n",
    "\n",
    "# Remove any \"Explanation:\" text\n",
    "analysis.content = analysis.content.split(\"Explanation:\")[0].strip()\n",
    "\n",
    "# Parse each section\n",
    "current_section = None\n",
    "for line in analysis.content.split('\\n'):\n",
    "    line = line.strip()\n",
    "    if not line:\n",
    "        continue\n",
    "        \n",
    "    if line.startswith('Tables:'):\n",
    "        current_section = 'tables'\n",
    "        tables = line.split(':')[1].strip()\n",
    "        components['tables'] = [t.strip() for t in tables.split(',') if t.strip()]\n",
    "    elif line.startswith('Columns:'):\n",
    "        current_section = 'columns'\n",
    "        cols = line.split(':')[1].strip()\n",
    "        for col_part in cols.split(','):\n",
    "            col_part = col_part.strip()\n",
    "            if col_part:\n",
    "                if '(' in col_part and ')' in col_part:\n",
    "                    agg = col_part.split('(')[0].strip().upper()\n",
    "                    col = col_part.split('(')[1].split(')')[0].strip()\n",
    "                    components['columns'].append((agg, col))\n",
    "                else:\n",
    "                    components['columns'].append((None, col_part))\n",
    "    elif line.startswith('Filters:'):\n",
    "        current_section = 'filters'\n",
    "        filters = line.split(':')[1].strip()\n",
    "        components['filters'] = [f.strip() for f in filters.split(',') if f.strip()]\n",
    "    elif line.startswith('Joins:'):\n",
    "        current_section = 'joins'\n",
    "        joins = line.split(':')[1].strip()\n",
    "        components['joins'] = [j.strip() for j in joins.split(',') if j.strip()]\n",
    "    elif line.startswith('GroupBy:'):\n",
    "        current_section = 'group_by'\n",
    "        group_bys = line.split(':')[1].strip()\n",
    "        components['group_by'] = [g.strip() for g in group_bys.split(',') if g.strip()]\n",
    "    elif current_section:\n",
    "        # Handle multi-line sections\n",
    "        if current_section == 'tables':\n",
    "            components['tables'].extend([t.strip() for t in line.split(',') if t.strip()])\n",
    "        elif current_section == 'columns':\n",
    "            for col_part in line.split(','):\n",
    "                col_part = col_part.strip()\n",
    "                if col_part:\n",
    "                    if '(' in col_part and ')' in col_part:\n",
    "                        agg = col_part.split('(')[0].strip().upper()\n",
    "                        col = col_part.split('(')[1].split(')')[0].strip()\n",
    "                        components['columns'].append((agg, col))\n",
    "                    else:\n",
    "                        components['columns'].append((None, col_part))\n",
    "        elif current_section == 'filters':\n",
    "            components['filters'].extend([f.strip() for f in line.split(',') if f.strip()])\n",
    "        elif current_section == 'joins':\n",
    "            components['joins'].extend([j.strip() for j in line.split(',') if j.strip()])\n",
    "        elif current_section == 'group_by':\n",
    "            components['group_by'].extend([g.strip() for g in line.split(',') if g.strip()])\n",
    "\n",
    "# Ensure schema is included in table names\n",
    "components['tables'] = [f\"SPURCHASE.{t.split('.')[-1]}\" if '.' not in t else t for t in components['tables']]\n",
    "\n",
    "# Ensure grouping columns are included in SELECT - CORRECTED VERSION\n",
    "for group_col in components['group_by']:\n",
    "    # Check if this exact (None, group_col) pair exists\n",
    "    col_exists = any(col == (None, group_col) for col in components['columns'])\n",
    "    # Check if group_col appears in any non-aggregated column reference\n",
    "    col_part_of_ref = any(group_col in col[1] for col in components['columns'] if col[0] is None)\n",
    "    \n",
    "    if not col_exists and not col_part_of_ref:\n",
    "        components['columns'].append((None, group_col))\n",
    "print(components)\n",
    "# return components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate SQL Query\n",
    "This code snipet takes the structured components extracted from a Large Language Model (LLM) and generates a clean, valid SQL query. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate clean SQL query from the analyzed components\"\"\"\n",
    "# Validate components\n",
    "if not components[\"tables\"]:\n",
    "    raise ValueError(\"No tables identified for SQL generation\")\n",
    "\n",
    "# Clean all components first\n",
    "def clean_component(component):\n",
    "    return component.replace('[', '').replace(']', '').strip()\n",
    "\n",
    "# Build SELECT clause - ensure GROUP BY columns are included\n",
    "select_parts = []\n",
    "\n",
    "# First add all GROUP BY columns to SELECT if they're not already there\n",
    "for group_col in components.get(\"group_by\", []):\n",
    "    group_col = clean_component(group_col)\n",
    "    if not any(col[1] == group_col for col in components[\"columns\"] if col[0] is None):\n",
    "        select_parts.append(group_col)\n",
    "\n",
    "# Then add the requested columns\n",
    "for agg, col in components[\"columns\"]:\n",
    "    col = clean_component(col)\n",
    "    if not col:\n",
    "        continue\n",
    "    if agg:\n",
    "        select_parts.append(f\"{agg}({col}) AS {agg}_{col}\")\n",
    "    else:\n",
    "        if col not in select_parts:  # Don't add duplicates\n",
    "            select_parts.append(col)\n",
    "\n",
    "if not select_parts:  # Default to all columns if none specified\n",
    "    select_parts.append(\"*\")\n",
    "\n",
    "select_clause = \", \".join(select_parts[1:])\n",
    "# print(\"SELECT BEFORE \"+select_clause)\n",
    "# print(select_parts)\n",
    "\n",
    "# Build FROM clause\n",
    "from_table = clean_component(components[\"tables\"][0])\n",
    "from_clause = from_table\n",
    "\n",
    "# Add joins only if they exist and are not empty\n",
    "join_clauses = []\n",
    "for join in components.get(\"joins\", []):\n",
    "    clean_join = clean_component(join)\n",
    "    if clean_join and clean_join != 'INNER JOIN':\n",
    "        join_clauses.append(f\"INNER JOIN SPURCHASE.LFA1 ON {clean_join}\")\n",
    "# print(\"INNER JOIN \"+clean_join)\n",
    "\n",
    "# Build WHERE clause\n",
    "where_clauses = []\n",
    "for filter_cond in components.get(\"filters\", []):\n",
    "    clean_filter = clean_component(filter_cond)\n",
    "    if clean_filter:\n",
    "        where_clauses.append(clean_filter)\n",
    "\n",
    "where_clause = \" AND \".join(where_clauses) if where_clauses else \"\"\n",
    "where_clause = where_clause.replace(\",\", \" AND\")\n",
    "# print(\"WHERE CLAUSE \"+where_clause)\n",
    "\n",
    "# Build GROUP BY clause\n",
    "group_by_columns = [clean_component(g) for g in components.get(\"group_by\", []) if clean_component(g)]\n",
    "group_by_clause = \", \".join(group_by_columns) if group_by_columns else \"\"\n",
    "\n",
    "# Construct the SQL\n",
    "sql = f\"SELECT {select_clause} FROM {from_clause}\"\n",
    "\n",
    "if join_clauses:\n",
    "    sql += \" \" + \" \".join(join_clauses)\n",
    "\n",
    "if where_clause:\n",
    "    sql += f\" WHERE {where_clause}\"\n",
    "\n",
    "if group_by_clause:\n",
    "    sql += f\" GROUP BY {group_by_clause}\"\n",
    "\n",
    "# Final formatting\n",
    "sql = sql.strip()\n",
    "if not sql.endswith(';'):\n",
    "    sql += ';'\n",
    "\n",
    "print(sql)\n",
    "#return sql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Execute the SQL Query\n",
    "This code executes a SQL query using a database connection (conn) and processes the result into a clean, tabular format using Pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Execute the generated SQL query and return results\n",
    "try:\n",
    "    cursor.execute(sql)\n",
    "    columns = [desc[0] for desc in cursor.description]\n",
    "    rows = cursor.fetchall()\n",
    "    results = pd.DataFrame(rows, columns=columns)\n",
    "    print(results)\n",
    "except Exception as e:\n",
    "    print(f\"Error executing SQL query: {e}\")\n",
    "finally:\n",
    "    cursor.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generate and Return the Final Response\n",
    "This code generates a natural language explanation of the SQL query results using a large language model (LLM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a natural language response from the query results\n",
    "if results.empty:\n",
    "    print(\"‚ùåNo results found for your query!\")\n",
    "\n",
    "prompt_template = \"\"\"Convert the following query results into a natural language response to the user's question. \n",
    "Keep the response concise but informative. Include relevant numbers and comparisons where appropriate.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Results:\n",
    "{results}\n",
    "\n",
    "Response:\n",
    "\"\"\"\n",
    "\n",
    "prompt = PromptTemplate.from_template(prompt_template).invoke({\n",
    "    \"question\": question,\n",
    "    \"results\": results.to_string()\n",
    "})\n",
    "\n",
    "response = anthropic.invoke(prompt)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
